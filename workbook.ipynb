{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EpvkBWyG9ISQ",
        "X8SHlUar9MqP",
        "-HOXGh1m9e4R",
        "rwEcZGr4_GrH",
        "7FG_WnVg_j95",
        "lEaJDzzP_1Jp",
        "2kmwYp7GAdiQ",
        "g--mBWOmE4Zr",
        "TL6pD76FFDCW",
        "IN7ta3QWFISi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - Symbolic Conditioned Generation\n"
      ],
      "metadata": {
        "id": "zkaF-yVv8Slc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processor"
      ],
      "metadata": {
        "id": "EpvkBWyG9ISQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4otmIUu8PZF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "class POP909DataProcessor:\n",
        "    \"\"\"\n",
        "    Data processor for attention-based global context approach.\n",
        "    This processor stores full chord sequences and creates training pairs\n",
        "    with focus positions for attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path: str, output_path: str, melody_segment_length: int = 32):\n",
        "        self.data_path = Path(data_path)\n",
        "        self.output_path = Path(output_path)\n",
        "        self.output_path.mkdir(exist_ok=True)\n",
        "\n",
        "        self.time_resolution = 0.125\n",
        "        self.min_chord_duration = 0.5\n",
        "        self.melody_segment_length = melody_segment_length\n",
        "\n",
        "        self.chord_vocab = set()\n",
        "        self.note_vocab = set()\n",
        "\n",
        "        self.processed_data = []\n",
        "\n",
        "    def parse_chord_file(self, chord_file_path: str) -> List[Tuple[float, float, str]]:\n",
        "        chords = []\n",
        "\n",
        "        try:\n",
        "            with open(chord_file_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    parts = line.split('\\t')\n",
        "                    if len(parts) >= 3:\n",
        "                        start_time, end_time, chord_symbol = parts[:3]\n",
        "                        if (float(end_time) - float(start_time)) >= self.min_chord_duration and chord_symbol != \"N\":\n",
        "                            simplified_chord = self.normalize_chord_symbol(chord_symbol)\n",
        "                            chords.append((float(start_time), float(end_time), simplified_chord))\n",
        "                            self.chord_vocab.add(simplified_chord)\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error parsing chord file {chord_file_path}: {e}\"\n",
        "            print(error_msg)\n",
        "\n",
        "        return chords\n",
        "\n",
        "    def extract_melody_track(self, midi_file_path: str) -> Optional[pretty_midi.Instrument]:\n",
        "        \"\"\"\n",
        "        Extract melody track from MIDI file\n",
        "        \"\"\"\n",
        "        midi_file_path = str(midi_file_path)\n",
        "        try:\n",
        "            midi_data = pretty_midi.PrettyMIDI(midi_file_path)\n",
        "            for instrument in midi_data.instruments:\n",
        "                if instrument.name and \"MELODY\" in instrument.name.upper():\n",
        "                    return instrument\n",
        "\n",
        "            if len(midi_data.instruments) > 0:\n",
        "                return midi_data.instruments[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading MIDI file {midi_file_path}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def quantize_melody(self, melody_track: pretty_midi.Instrument, song_duration: float) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Quantize melody to fixed time grid and extract features\n",
        "        \"\"\"\n",
        "        quantized_melody = []\n",
        "        time_steps = np.arange(0, song_duration, self.time_resolution)\n",
        "\n",
        "        for note in melody_track.notes:\n",
        "            start_step = int(np.round(note.start / self.time_resolution))\n",
        "            end_step = int(np.round(note.end / self.time_resolution))\n",
        "\n",
        "            if start_step >= len(time_steps) or end_step > len(time_steps):\n",
        "                continue\n",
        "\n",
        "            note_info = {\n",
        "                'start_time': time_steps[start_step],\n",
        "                'end_time': time_steps[end_step-1] if end_step > 0 else time_steps[start_step],\n",
        "                'pitch': note.pitch,\n",
        "                'velocity': note.velocity,\n",
        "                'duration': note.end - note.start\n",
        "            }\n",
        "            quantized_melody.append(note_info)\n",
        "            self.note_vocab.add(note.pitch)\n",
        "\n",
        "        return quantized_melody\n",
        "\n",
        "    def normalize_chord_symbol(self, chord_symbol: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize and simplify chord symbols\n",
        "        \"\"\"\n",
        "        if chord_symbol == 'N' or not chord_symbol or chord_symbol.strip() == '':\n",
        "            return 'N'\n",
        "\n",
        "        cleaned = chord_symbol.strip()\n",
        "        simplified = self.simplify_chord_symbol(cleaned)\n",
        "        return simplified\n",
        "\n",
        "    def process_song(self, song_folder: Path) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Process a single song with simplified chord vocabulary\n",
        "        \"\"\"\n",
        "        song_id = song_folder.name\n",
        "        midi_file_path = song_folder / f\"{song_id}.mid\"\n",
        "        chord_file_path = song_folder / \"chord_audio.txt\"\n",
        "\n",
        "        if not midi_file_path.exists() or not chord_file_path.exists():\n",
        "            print(f\"Skipping {song_id} due to missing files\")\n",
        "            return None\n",
        "\n",
        "        chords = self.parse_chord_file(chord_file_path)\n",
        "        if len(chords) < 4:\n",
        "            print(f\"Skipping {song_id} - not enough chords ({len(chords)})\")\n",
        "            return None\n",
        "\n",
        "        combined_track = self.extract_melody_track(midi_file_path)\n",
        "\n",
        "        song_duration = max(chords[-1][1], combined_track.notes[-1].end)\n",
        "\n",
        "        quantized_melody = self.quantize_melody(combined_track, song_duration)\n",
        "        if not quantized_melody:\n",
        "            print(f\"No quantized melody for song {song_id}\")\n",
        "            return None\n",
        "\n",
        "        melody_segments = self.create_chord_aligned_segments(chords, quantized_melody)\n",
        "        if not melody_segments:\n",
        "            print(f\"No melody segments for song {song_id}\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'song_id': song_id,\n",
        "            'melody_segments': melody_segments,\n",
        "            'total_duration': song_duration,\n",
        "            'num_chords': len(chords),\n",
        "            'num_melody_notes': len(quantized_melody),\n",
        "            'full_chord_sequence': [chord[2] for chord in chords]\n",
        "        }\n",
        "\n",
        "    def process_dataset(self) -> None:\n",
        "        \"\"\"\n",
        "        Process all songs in the dataset and save the melody segments.\n",
        "        \"\"\"\n",
        "        pop909_folder = self.data_path / \"POP909\"\n",
        "        if not pop909_folder.exists():\n",
        "            print(f\"POP909 folder not found at {pop909_folder}\")\n",
        "            return\n",
        "\n",
        "        song_folders = [f for f in pop909_folder.iterdir() if f.is_dir()]\n",
        "        print(f\"Found {len(song_folders)} song folders in POP909 dataset\")\n",
        "\n",
        "        successful_songs = 0\n",
        "        for song_folder in sorted(song_folders):\n",
        "            song_data = self.process_song(song_folder)\n",
        "            if song_data:\n",
        "                self.processed_data.append(song_data)\n",
        "                successful_songs += 1\n",
        "\n",
        "                if successful_songs % 50 == 0:\n",
        "                    print(f\"Processed {successful_songs}/{len(song_folders)} songs\")\n",
        "\n",
        "        print(f\"Completed processing. {successful_songs}/{len(song_folders)} songs processed successfully\")\n",
        "\n",
        "    def create_vocabularies(self) -> Dict:\n",
        "        chord_vocab_list = ['<PAD>', '<START>', '<END>', '<UNK>'] + sorted(list(self.chord_vocab))\n",
        "        note_vocab_list = list(range(0, 128))\n",
        "\n",
        "        vocab_data = {\n",
        "            'chord_vocab': {chord: idx for idx, chord in enumerate(chord_vocab_list)},\n",
        "            'note_vocab': {note: idx for idx, note in enumerate(note_vocab_list)},\n",
        "            'idx_to_chord': {idx: chord for idx, chord in enumerate(chord_vocab_list)},\n",
        "            'idx_to_note': {idx: note for idx, note in enumerate(note_vocab_list)}\n",
        "        }\n",
        "\n",
        "        with open(self.output_path / \"vocabularies.json\", 'w') as f:\n",
        "            json.dump(vocab_data, f, indent=2)\n",
        "\n",
        "        print(f\"Chord vocabulary size: {len(chord_vocab_list)}\")\n",
        "        print(f\"Note vocabulary size: {len(note_vocab_list)}\")\n",
        "\n",
        "        return vocab_data\n",
        "\n",
        "    def save_processed_data(self) -> None:\n",
        "        with open(self.output_path / \"chord_melody_data.pkl\", 'wb') as f:\n",
        "            pickle.dump(self.processed_data, f)\n",
        "\n",
        "        with open(self.output_path / \"chord_melody_data.json\", 'w') as f:\n",
        "            json.dump(self.processed_data, f, indent=2)\n",
        "\n",
        "        total_segments = sum(len(song['melody_segments']) for song in self.processed_data)\n",
        "        avg_segments_per_song = total_segments / len(self.processed_data) if self.processed_data else 0\n",
        "\n",
        "        stats = {\n",
        "            'total_songs': len(self.processed_data),\n",
        "            'total_melody_segments': total_segments,\n",
        "            'avg_segments_per_song': avg_segments_per_song,\n",
        "            'unique_chords': len(self.chord_vocab),\n",
        "            'note_range': [min(self.note_vocab), max(self.note_vocab)] if self.note_vocab else [0, 127],\n",
        "            'melody_segment_length': self.melody_segment_length\n",
        "        }\n",
        "\n",
        "        with open(self.output_path / \"dataset_stats.json\", 'w') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "\n",
        "        print(f\"Processed data saved to {self.output_path}\")\n",
        "        print(f\"Dataset statistics: {stats}\")\n",
        "\n",
        "    def create_training_sequences(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Create training sequences with proper chord-melody alignment\n",
        "        \"\"\"\n",
        "        training_sequences = []\n",
        "\n",
        "        for song in self.processed_data:\n",
        "            for segment in song['melody_segments']:\n",
        "                for chord_pair in segment['chord_melody_pairs']:\n",
        "\n",
        "                    chord_sequence = segment['full_chord_sequence']\n",
        "                    current_chord_pos = chord_pair['chord_position_in_segment']\n",
        "\n",
        "                    melody_notes = []\n",
        "                    for note in chord_pair['notes']:\n",
        "                        melody_notes.append({\n",
        "                            'pitch': note['pitch'],\n",
        "                            'start_time': note['start_in_chord'],\n",
        "                            'duration': note['end_in_chord'] - note['start_in_chord'],\n",
        "                            'velocity': note['velocity']\n",
        "                        })\n",
        "\n",
        "                    training_example = {\n",
        "                        'full_chord_sequence': chord_sequence,\n",
        "                        'chord_durations': segment['chord_durations'],\n",
        "                        'focus_position': current_chord_pos,\n",
        "                        'target_chord': chord_pair['chord'],\n",
        "                        'chord_duration': chord_pair['chord_duration'],\n",
        "                        'output_melody': melody_notes,\n",
        "                        'song_id': song['song_id'],\n",
        "                        'segment_id': segment['segment_id'],\n",
        "                        'timing_context': {\n",
        "                            'bpm_estimate': 60.0 / (chord_pair['chord_duration'] / 2),\n",
        "                            'time_signature': '4/4',\n",
        "                            'chord_position_in_song': segment['segment_start_time'] / song['total_duration']\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                    if len(melody_notes) > 0:\n",
        "                        training_sequences.append(training_example)\n",
        "\n",
        "        with open(self.output_path / \"training_sequences.pkl\", 'wb') as f:\n",
        "            pickle.dump(training_sequences, f)\n",
        "\n",
        "        print(f\"Created {len(training_sequences)} chord-aligned training sequences\")\n",
        "        return training_sequences\n",
        "\n",
        "\n",
        "    def simplify_chord_symbol(self, chord_symbol: str) -> str:\n",
        "        \"\"\"\n",
        "        Simplify chord symbols to reduce vocabulary size and improve generalization.\n",
        "\n",
        "        Maps complex chords to basic chord types:\n",
        "        - All major variations -> maj\n",
        "        - All minor variations -> min\n",
        "        - All dominant 7th variations -> 7\n",
        "        - All major 7th variations -> maj7\n",
        "        - All minor 7th variations -> min7\n",
        "        - All diminished variations -> dim\n",
        "        - All augmented variations -> aug\n",
        "        - All suspended variations -> sus\n",
        "        \"\"\"\n",
        "\n",
        "        if chord_symbol == 'N' or not chord_symbol or chord_symbol.strip() == '':\n",
        "            return 'N'\n",
        "\n",
        "        if ':' not in chord_symbol:\n",
        "            return chord_symbol\n",
        "\n",
        "        root, quality = chord_symbol.split(':', 1)\n",
        "\n",
        "        simplified_quality = None\n",
        "\n",
        "        if any(pattern in quality for pattern in ['maj7', 'M7']):\n",
        "            if any(pattern in quality for pattern in ['min', 'm7', 'minmaj']):\n",
        "                simplified_quality = 'minmaj7'\n",
        "            else:\n",
        "                simplified_quality = 'maj7'\n",
        "        elif 'maj' in quality:\n",
        "            simplified_quality = 'maj'\n",
        "\n",
        "        elif any(pattern in quality for pattern in ['min7', 'm7']) and 'maj' not in quality:\n",
        "            simplified_quality = 'min7'\n",
        "        elif 'min' in quality or quality.startswith('m'):\n",
        "            simplified_quality = 'min'\n",
        "\n",
        "        elif any(pattern in quality for pattern in ['7', '9', '11', '13']) and 'maj' not in quality and 'min' not in quality:\n",
        "            simplified_quality = '7'\n",
        "\n",
        "        elif any(pattern in quality for pattern in ['dim', 'hdim', 'ø']):\n",
        "            if '7' in quality or 'hdim' in quality:\n",
        "                simplified_quality = 'dim7'\n",
        "            else:\n",
        "                simplified_quality = 'dim'\n",
        "\n",
        "        elif 'aug' in quality or '+' in quality:\n",
        "            simplified_quality = 'aug'\n",
        "\n",
        "        elif any(pattern in quality for pattern in ['sus2', 'sus4', 'sus']):\n",
        "            if 'sus2' in quality:\n",
        "                simplified_quality = 'sus2'\n",
        "            else:\n",
        "                simplified_quality = 'sus4'\n",
        "\n",
        "        elif '6' in quality and 'min' not in quality:\n",
        "            simplified_quality = 'maj'\n",
        "        elif '6' in quality and 'min' in quality:\n",
        "            simplified_quality = 'min'\n",
        "\n",
        "        if simplified_quality is None:\n",
        "            if quality == '' or quality.isdigit():\n",
        "                simplified_quality = 'maj'\n",
        "            else:\n",
        "                simplified_quality = 'maj'\n",
        "\n",
        "        return f\"{root}:{simplified_quality}\"\n",
        "\n",
        "    def create_chord_aligned_segments(self, chords: List[Tuple[float, float, str]], melody: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Create training segments aligned to chord boundaries for better timing relationships.\n",
        "        Each segment represents a sequence of chords with their corresponding melody notes.\n",
        "        \"\"\"\n",
        "        segments = []\n",
        "\n",
        "        if not chords or not melody:\n",
        "            return segments\n",
        "\n",
        "        min_chords_per_segment = 4\n",
        "        max_chords_per_segment = 12\n",
        "        overlap_chords = 2\n",
        "\n",
        "        chord_idx = 0\n",
        "        while chord_idx < len(chords):\n",
        "            segment_end_idx = min(chord_idx + max_chords_per_segment, len(chords))\n",
        "            segment_end_idx = min(chord_idx + max_chords_per_segment, len(chords))\n",
        "\n",
        "            if segment_end_idx - chord_idx < min_chords_per_segment and segment_end_idx < len(chords):\n",
        "                segment_end_idx = min(chord_idx + min_chords_per_segment, len(chords))\n",
        "\n",
        "            segment_chords = chords[chord_idx:segment_end_idx]\n",
        "            segment_start_time = segment_chords[0][0]\n",
        "            segment_end_time = segment_chords[-1][1]\n",
        "\n",
        "            chord_melody_pairs = []\n",
        "\n",
        "            for local_idx, (chord_start, chord_end, chord_symbol) in enumerate(segment_chords):\n",
        "                chord_notes = []\n",
        "\n",
        "                for note in melody:\n",
        "                    note_start = note['start_time']\n",
        "                    note_end = note['end_time']\n",
        "\n",
        "                    if not (note_end <= chord_start or note_start >= chord_end):\n",
        "                        overlap_start = max(note_start, chord_start)\n",
        "                        overlap_end = min(note_end, chord_end)\n",
        "                        overlap_duration = overlap_end - overlap_start\n",
        "                        note_duration = note_end - note_start\n",
        "\n",
        "                        if note_duration > 0 and overlap_duration / note_duration > 0.5:\n",
        "                            relative_note = {\n",
        "                                'pitch': note['pitch'],\n",
        "                                'start_in_chord': max(0, note_start - chord_start),\n",
        "                                'end_in_chord': min(chord_end - chord_start, note_end - chord_start),\n",
        "                                'duration': note.get('duration', note_end - note_start),\n",
        "                                'velocity': note.get('velocity', 80),\n",
        "                                'chord_duration': chord_end - chord_start\n",
        "                            }\n",
        "                            chord_notes.append(relative_note)\n",
        "\n",
        "                chord_melody_pairs.append({\n",
        "                    'chord': chord_symbol,\n",
        "                    'chord_duration': chord_end - chord_start,\n",
        "                    'chord_position_in_segment': local_idx,\n",
        "                    'notes': chord_notes,\n",
        "                    'absolute_start_time': chord_start,\n",
        "                    'absolute_end_time': chord_end\n",
        "                })\n",
        "\n",
        "            total_notes = sum(len(pair['notes']) for pair in chord_melody_pairs)\n",
        "            if total_notes > 0 and len(segment_chords) >= min_chords_per_segment:\n",
        "                segment = {\n",
        "                    'chord_melody_pairs': chord_melody_pairs,\n",
        "                    'full_chord_sequence': [pair['chord'] for pair in chord_melody_pairs],\n",
        "                    'chord_durations': [pair['chord_duration'] for pair in chord_melody_pairs],\n",
        "                    'segment_start_time': segment_start_time,\n",
        "                    'segment_end_time': segment_end_time,\n",
        "                    'total_duration': segment_end_time - segment_start_time,\n",
        "                    'num_chords': len(segment_chords),\n",
        "                    'total_notes': total_notes,\n",
        "                    'segment_id': len(segments)\n",
        "                }\n",
        "                segments.append(segment)\n",
        "\n",
        "            step_size = max_chords_per_segment - overlap_chords\n",
        "            chord_idx += step_size\n",
        "\n",
        "            if chord_idx >= len(chords) - min_chords_per_segment:\n",
        "                break\n",
        "\n",
        "        return segments\n",
        "\n",
        "def process_dataset(dataset_path: str, output_path: str, melody_segment_length: int = 32):\n",
        "    processor = POP909DataProcessor(dataset_path, output_path, melody_segment_length)\n",
        "    processor.process_dataset()\n",
        "    processor.create_vocabularies()\n",
        "    processor.save_processed_data()\n",
        "    processor.create_training_sequences()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Builder"
      ],
      "metadata": {
        "id": "X8SHlUar9MqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class ChordMelodyDataset(Dataset):\n",
        "    def __init__(self, data_path: str, vocab_path: str, max_melody_length: int = 16, max_chord_length: int = 12):\n",
        "        with open(data_path, 'rb') as f:\n",
        "            self.training_sequences = pickle.load(f)\n",
        "        with open(vocab_path, 'r') as f:\n",
        "            self.vocabularies = json.load(f)\n",
        "\n",
        "        self.chord_to_idx = self.vocabularies['chord_vocab']\n",
        "        self.note_to_idx = self.vocabularies['note_vocab']\n",
        "        self.max_melody_length = max_melody_length\n",
        "        self.max_chord_length = max_chord_length\n",
        "\n",
        "        self.training_sequences = [seq for seq in self.training_sequences\n",
        "                                 if len(seq['output_melody']) <= max_melody_length\n",
        "                                 and len(seq['full_chord_sequence']) <= max_chord_length]\n",
        "\n",
        "        print(f\"Loaded {len(self.training_sequences)} chord-aligned training sequences\")\n",
        "        if self.training_sequences:\n",
        "            avg_chord_len = np.mean([len(seq['full_chord_sequence']) for seq in self.training_sequences])\n",
        "            avg_melody_len = np.mean([len(seq['output_melody']) for seq in self.training_sequences])\n",
        "            print(f\"Average chord sequence length: {avg_chord_len:.1f}\")\n",
        "            print(f\"Average melody length: {avg_melody_len:.1f}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.training_sequences[idx]\n",
        "\n",
        "        chord_indices = []\n",
        "        for chord in sequence['full_chord_sequence']:\n",
        "            chord_indices.append(self.chord_to_idx.get(chord, self.chord_to_idx['<UNK>']))\n",
        "\n",
        "        chord_durations = sequence['chord_durations']\n",
        "        max_duration = max(chord_durations) if chord_durations else 4.0\n",
        "        normalized_durations = [d / max_duration for d in chord_durations]\n",
        "\n",
        "        original_chord_length = len(chord_indices)\n",
        "        while len(chord_indices) < self.max_chord_length:\n",
        "            chord_indices.append(self.chord_to_idx['<PAD>'])\n",
        "            normalized_durations.append(0.0)\n",
        "\n",
        "        focus_position = min(sequence['focus_position'], original_chord_length - 1)\n",
        "\n",
        "        target_chord_duration = sequence['chord_duration']\n",
        "\n",
        "        melody_pitches, melody_starts, melody_durations = [], [], []\n",
        "        for note in sequence['output_melody']:\n",
        "            melody_pitches.append(note['pitch'])\n",
        "            melody_starts.append(note['start_time'] / target_chord_duration)\n",
        "            melody_durations.append(note['duration'] / target_chord_duration)\n",
        "\n",
        "        actual_melody_length = len(melody_pitches)\n",
        "        while len(melody_pitches) < self.max_melody_length:\n",
        "            melody_pitches.append(0)\n",
        "            melody_starts.append(0.0)\n",
        "            melody_durations.append(0.0)\n",
        "\n",
        "        chord_mask = [1 if i < original_chord_length else 0 for i in range(self.max_chord_length)]\n",
        "        melody_mask = [1 if i < actual_melody_length else 0 for i in range(self.max_melody_length)]\n",
        "\n",
        "        return {\n",
        "            'full_chord_sequence': torch.tensor(chord_indices, dtype=torch.long),\n",
        "            'chord_durations': torch.tensor(normalized_durations, dtype=torch.float32),\n",
        "            'chord_mask': torch.tensor(chord_mask, dtype=torch.bool),\n",
        "            'focus_position': torch.tensor(focus_position, dtype=torch.long),\n",
        "            'target_chord_duration': torch.tensor(target_chord_duration, dtype=torch.float32),\n",
        "            'melody_pitch': torch.tensor(melody_pitches, dtype=torch.long),\n",
        "            'melody_start': torch.tensor(melody_starts, dtype=torch.float32),\n",
        "            'melody_duration': torch.tensor(melody_durations, dtype=torch.float32),\n",
        "            'melody_mask': torch.tensor(melody_mask, dtype=torch.bool),\n",
        "            'chord_length': torch.tensor(original_chord_length, dtype=torch.long),\n",
        "            'melody_length': torch.tensor(actual_melody_length, dtype=torch.long),\n",
        "            'song_id': sequence['song_id']\n",
        "        }"
      ],
      "metadata": {
        "id": "yxOrEYAt9Rsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "-HOXGh1m9e4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import math\n",
        "\n",
        "class AttentionChordToMelodyTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                chord_vocab_size: int,\n",
        "                note_vocab_size: int,\n",
        "                d_model: int = 256,\n",
        "                nhead: int = 8,\n",
        "                num_layers: int = 6,\n",
        "                max_melody_length: int = 32,\n",
        "                max_chord_length: int = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_melody_length = max_melody_length\n",
        "        self.max_chord_length = max_chord_length\n",
        "\n",
        "        self.chord_embedding = nn.Embedding(chord_vocab_size, d_model)\n",
        "        self.note_embedding = nn.Embedding(note_vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_melody_length, d_model)\n",
        "        self.chord_position_embedding = nn.Embedding(max_chord_length, d_model)\n",
        "\n",
        "        self.segment_position_embedding = nn.Linear(1, d_model)\n",
        "\n",
        "        self.chord_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model,\n",
        "                nhead,\n",
        "                dim_feedforward=d_model * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=num_layers // 2\n",
        "        )\n",
        "\n",
        "        self.melody_decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(\n",
        "                d_model,\n",
        "                nhead,\n",
        "                dim_feedforward=d_model * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=num_layers // 2\n",
        "        )\n",
        "\n",
        "        self.focus_attention = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "        self.focus_projection = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.pitch_head = nn.Linear(d_model, note_vocab_size)\n",
        "        self.duration_head = nn.Linear(d_model, 1)\n",
        "        self.start_head = nn.Linear(d_model, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights properly for transformer\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, std=0.02)\n",
        "\n",
        "    def create_distance_attention_mask(self, seq_len: int, focus_positions: torch.Tensor, focus_window: int = 8):\n",
        "        \"\"\"\n",
        "        Create attention mask that focuses on nearby chords with some global context\n",
        "        \"\"\"\n",
        "        batch_size = focus_positions.size(0)\n",
        "        device = focus_positions.device\n",
        "\n",
        "        mask = torch.zeros(batch_size, seq_len, seq_len, device=device)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            focus_pos = focus_positions[b].item()\n",
        "\n",
        "            distances = torch.arange(seq_len, device=device).float()\n",
        "            focus_distances = torch.abs(distances - focus_pos)\n",
        "\n",
        "            local_weights = torch.exp(-focus_distances / (focus_window / 2))\n",
        "\n",
        "            global_weights = torch.where(\n",
        "                (torch.arange(seq_len, device=device) % 4) == 0,\n",
        "                torch.ones(seq_len, device=device) * 0.3,\n",
        "                torch.zeros(seq_len, device=device)\n",
        "            )\n",
        "\n",
        "            combined_weights = local_weights + global_weights\n",
        "            combined_weights = combined_weights / combined_weights.sum()\n",
        "\n",
        "            for i in range(seq_len):\n",
        "                mask[b, i, :] = combined_weights\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def _align_simultaneous_notes(self, melody, tolerance=0.1):\n",
        "        \"\"\"\n",
        "        Align notes that should be played simultaneously\n",
        "        \"\"\"\n",
        "        if len(melody) <= 1:\n",
        "            return melody\n",
        "\n",
        "        sorted_melody = sorted(melody, key=lambda x: x['start_time'])\n",
        "        aligned_melody = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(sorted_melody):\n",
        "            current_note = sorted_melody[i].copy()\n",
        "            simultaneous_notes = [current_note]\n",
        "\n",
        "            j = i + 1\n",
        "            while j < len(sorted_melody):\n",
        "                next_note = sorted_melody[j]\n",
        "                time_diff = abs(next_note['start_time'] - current_note['start_time'])\n",
        "\n",
        "                if time_diff <= tolerance:\n",
        "                    simultaneous_notes.append(next_note.copy())\n",
        "                    j += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            if len(simultaneous_notes) > 1:\n",
        "                avg_start_time = sum(note['start_time'] for note in simultaneous_notes) / len(simultaneous_notes)\n",
        "\n",
        "                print(f\"  Aligning {len(simultaneous_notes)} simultaneous notes at {avg_start_time:.2f}s\")\n",
        "                print(f\"    Pitches: {[note['pitch'] for note in simultaneous_notes]}\")\n",
        "\n",
        "                for note in simultaneous_notes:\n",
        "                    note['start_time'] = avg_start_time\n",
        "\n",
        "                avg_duration = sum(note['duration'] for note in simultaneous_notes) / len(simultaneous_notes)\n",
        "                for note in simultaneous_notes:\n",
        "                    note['duration'] = avg_duration\n",
        "\n",
        "            aligned_melody.extend(simultaneous_notes)\n",
        "            i = j\n",
        "\n",
        "        return aligned_melody\n",
        "\n",
        "    def _quantize_note_timing(self, melody, beat_duration=0.25):\n",
        "        \"\"\"\n",
        "        Quantize note timing to musical beats for cleaner rhythm\n",
        "        \"\"\"\n",
        "        print(f\"  Quantizing notes to {beat_duration:.3f}s grid...\")\n",
        "\n",
        "        quantized_melody = []\n",
        "\n",
        "        for note in melody:\n",
        "            quantized_note = note.copy()\n",
        "            quantized_start = round(note['start_time'] / beat_duration) * beat_duration\n",
        "            raw_duration = note['duration']\n",
        "            musical_durations = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 4.0]\n",
        "            closest_duration = min(musical_durations, key=lambda x: abs(x - raw_duration))\n",
        "            quantized_note['start_time'] = quantized_start\n",
        "            quantized_note['duration'] = closest_duration\n",
        "            quantized_melody.append(quantized_note)\n",
        "\n",
        "        return quantized_melody\n",
        "\n",
        "    def _remove_timing_conflicts(self, melody, min_gap=0.05):\n",
        "        \"\"\"\n",
        "        Remove notes that create timing conflicts while preserving harmonies\n",
        "        \"\"\"\n",
        "        if len(melody) <= 1:\n",
        "            return melody\n",
        "\n",
        "        sorted_melody = sorted(melody, key=lambda x: (x['start_time'], x['pitch']))\n",
        "        cleaned_melody = []\n",
        "\n",
        "        for note in sorted_melody:\n",
        "            should_add = True\n",
        "\n",
        "            for existing_note in cleaned_melody:\n",
        "                existing_end = existing_note['start_time'] + existing_note['duration']\n",
        "                note_start = note['start_time']\n",
        "                note_end = note['start_time'] + note['duration']\n",
        "\n",
        "                if abs(existing_note['start_time'] - note_start) < 0.01:\n",
        "                    continue\n",
        "\n",
        "                elif (note_start < existing_end - min_gap and\n",
        "                      note_end > existing_note['start_time'] + min_gap):\n",
        "\n",
        "                    print(f\"Removing conflicting note: pitch {note['pitch']} at {note_start:.2f}s\")\n",
        "                    should_add = False\n",
        "                    break\n",
        "\n",
        "            if should_add:\n",
        "                cleaned_melody.append(note)\n",
        "\n",
        "        return cleaned_melody\n",
        "\n",
        "    def _enhance_note_timing(self, melody):\n",
        "        \"\"\"\n",
        "        Master function to enhance note timing for better musical quality\n",
        "        \"\"\"\n",
        "        if not melody:\n",
        "            return melody\n",
        "\n",
        "        print(f\"Enhancing timing for {len(melody)} notes...\")\n",
        "\n",
        "        melody = self._remove_timing_conflicts(melody, min_gap=0.05)\n",
        "        print(f\"  After conflict removal: {len(melody)} notes\")\n",
        "\n",
        "        melody = self._align_simultaneous_notes(melody, tolerance=0.15)\n",
        "        print(f\"  After alignment: {len(melody)} notes\")\n",
        "\n",
        "        melody = sorted(melody, key=lambda x: x['start_time'])\n",
        "\n",
        "        self._report_simultaneous_groups(melody)\n",
        "\n",
        "        return melody\n",
        "\n",
        "    def _report_simultaneous_groups(self, melody):\n",
        "        \"\"\"Report groups of simultaneous notes for debugging\"\"\"\n",
        "        simultaneous_groups = []\n",
        "        i = 0\n",
        "\n",
        "        while i < len(melody):\n",
        "            current_time = melody[i]['start_time']\n",
        "            group = [melody[i]]\n",
        "\n",
        "            j = i + 1\n",
        "            while j < len(melody) and abs(melody[j]['start_time'] - current_time) < 0.01:\n",
        "                group.append(melody[j])\n",
        "                j += 1\n",
        "\n",
        "            if len(group) > 1:\n",
        "                pitches = [note['pitch'] for note in group]\n",
        "                simultaneous_groups.append((current_time, pitches))\n",
        "\n",
        "            i = j\n",
        "\n",
        "        if simultaneous_groups:\n",
        "            print(f\"  Found {len(simultaneous_groups)} simultaneous note groups:\")\n",
        "            for time, pitches in simultaneous_groups[:5]:  # Show first 5\n",
        "                print(f\"    {time:.2f}s: pitches {pitches}\")\n",
        "            if len(simultaneous_groups) > 5:\n",
        "                print(f\"    ... and {len(simultaneous_groups) - 5} more groups\")\n",
        "\n",
        "    def _calculate_song_bpm(self, chord_times):\n",
        "        \"\"\"Calculate BPM from chord timing for better quantization\"\"\"\n",
        "        if len(chord_times) < 4:\n",
        "            return 120\n",
        "\n",
        "        durations = [end - start for start, end in chord_times]\n",
        "        avg_chord_duration = sum(durations) / len(durations)\n",
        "\n",
        "        estimated_bpm = 60.0 / avg_chord_duration\n",
        "\n",
        "        common_bpms = [60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 180]\n",
        "        closest_bpm = min(common_bpms, key=lambda x: abs(x - estimated_bpm))\n",
        "\n",
        "        print(f\"  Estimated BPM: {estimated_bpm:.1f} -> Using: {closest_bpm}\")\n",
        "        return closest_bpm\n",
        "\n",
        "    def _enhance_note_timing_with_bpm(self, melody, beat_duration):\n",
        "        \"\"\"Enhanced timing with BPM awareness\"\"\"\n",
        "        if not melody:\n",
        "            return melody\n",
        "\n",
        "        print(f\"Enhancing timing for {len(melody)} notes (beat={beat_duration:.3f}s)...\")\n",
        "\n",
        "        melody = self._remove_timing_conflicts(melody)\n",
        "\n",
        "        melody = self._align_simultaneous_notes(melody, tolerance=beat_duration * 0.5)\n",
        "\n",
        "        quantized_melody = []\n",
        "        for note in melody:\n",
        "            quantized_note = note.copy()\n",
        "\n",
        "            closest_beat_time = round(note['start_time'] / beat_duration) * beat_duration\n",
        "            time_diff = abs(note['start_time'] - closest_beat_time)\n",
        "\n",
        "            if time_diff < beat_duration * 0.3:\n",
        "                quantized_note['start_time'] = closest_beat_time\n",
        "\n",
        "            quantized_melody.append(quantized_note)\n",
        "\n",
        "        self._report_simultaneous_groups(quantized_melody)\n",
        "        return sorted(quantized_melody, key=lambda x: x['start_time'])\n",
        "\n",
        "    def forward(self, full_chord_sequence, chord_mask, focus_positions, chord_durations=None,\n",
        "                melody_pitch=None, training=True):\n",
        "        batch_size = full_chord_sequence.size(0)\n",
        "        chord_seq_len = full_chord_sequence.size(1)\n",
        "\n",
        "        chord_embedded = self.chord_embedding(full_chord_sequence)\n",
        "\n",
        "        chord_positions = torch.arange(chord_seq_len, device=full_chord_sequence.device)\n",
        "        chord_pos_emb = self.chord_position_embedding(chord_positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        chord_embedded = chord_embedded + chord_pos_emb\n",
        "\n",
        "        if chord_durations is not None:\n",
        "            duration_emb = chord_durations.unsqueeze(-1).expand(-1, -1, self.d_model) * 0.1\n",
        "            chord_embedded = chord_embedded + duration_emb\n",
        "\n",
        "        chord_embedded = self.dropout(chord_embedded)\n",
        "\n",
        "        padding_mask = ~chord_mask\n",
        "\n",
        "        chord_encoded = self.chord_encoder(\n",
        "            chord_embedded,\n",
        "            src_key_padding_mask=padding_mask\n",
        "        )\n",
        "\n",
        "        focus_context, focus_weights = self.focus_attention(\n",
        "            chord_encoded, chord_encoded, chord_encoded,\n",
        "            key_padding_mask=padding_mask\n",
        "        )\n",
        "\n",
        "        chord_encoded = chord_encoded + self.focus_projection(focus_context)\n",
        "        chord_encoded = self.layer_norm(chord_encoded)\n",
        "\n",
        "        if training:\n",
        "            target_melody = melody_pitch[:, :-1]\n",
        "\n",
        "            melody_embedded = self.note_embedding(target_melody)\n",
        "            melody_seq_len = target_melody.size(1)\n",
        "\n",
        "            melody_positions = torch.arange(melody_seq_len, device=target_melody.device)\n",
        "            melody_pos_emb = self.position_embedding(melody_positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            melody_embedded = melody_embedded + melody_pos_emb\n",
        "            melody_embedded = self.dropout(melody_embedded)\n",
        "\n",
        "            causal_mask = torch.triu(torch.ones(melody_seq_len, melody_seq_len, device=target_melody.device), diagonal=1).bool()\n",
        "\n",
        "            decoded = self.melody_decoder(\n",
        "                melody_embedded,\n",
        "                chord_encoded,\n",
        "                tgt_mask=causal_mask,\n",
        "                memory_key_padding_mask=padding_mask\n",
        "            )\n",
        "\n",
        "            pitch_logits = self.pitch_head(decoded)\n",
        "            duration_pred = self.duration_head(decoded).squeeze(-1)\n",
        "            start_pred = self.start_head(decoded).squeeze(-1)\n",
        "\n",
        "            return pitch_logits, duration_pred, start_pred\n",
        "\n",
        "        else:\n",
        "            generated_sequence = []\n",
        "            current_input = torch.zeros(batch_size, 1, dtype=torch.long, device=full_chord_sequence.device)\n",
        "\n",
        "            for i in range(self.max_melody_length):\n",
        "                melody_embedded = self.note_embedding(current_input)\n",
        "                melody_seq_len = current_input.size(1)\n",
        "\n",
        "                melody_positions = torch.arange(melody_seq_len, device=current_input.device)\n",
        "                melody_pos_emb = self.position_embedding(melody_positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "                melody_embedded = melody_embedded + melody_pos_emb\n",
        "                melody_embedded = self.dropout(melody_embedded)\n",
        "\n",
        "                causal_mask = torch.triu(torch.ones(melody_seq_len, melody_seq_len, device=current_input.device), diagonal=1).bool()\n",
        "\n",
        "                decoded = self.melody_decoder(\n",
        "                    melody_embedded,\n",
        "                    chord_encoded,\n",
        "                    tgt_mask=causal_mask,\n",
        "                    memory_key_padding_mask=padding_mask\n",
        "                )\n",
        "\n",
        "                last_decoded = decoded[:, -1:]\n",
        "                pitch_logits = self.pitch_head(last_decoded)\n",
        "                duration_pred = self.duration_head(last_decoded).squeeze(-1)\n",
        "                start_pred = self.start_head(last_decoded).squeeze(-1)\n",
        "\n",
        "                pitch_probs = torch.softmax(pitch_logits.squeeze(1), dim=-1)\n",
        "                next_pitch = torch.multinomial(pitch_probs, 1)\n",
        "\n",
        "                generated_sequence.append({\n",
        "                    'pitch': next_pitch,\n",
        "                    'duration': duration_pred,\n",
        "                    'start': start_pred\n",
        "                })\n",
        "\n",
        "                if next_pitch.item() == 0 or len(generated_sequence) >= self.max_melody_length:\n",
        "                    break\n",
        "\n",
        "                current_input = torch.cat([current_input, next_pitch], dim=1)\n",
        "\n",
        "            return generated_sequence\n",
        "\n",
        "    def generate_full_song_melody(self, full_chord_sequence, vocab_path, segment_length=16, overlap=4,\n",
        "                    target_density=0.6, density_window=8, temperature=1.0,\n",
        "                    original_midi_path=None):\n",
        "        \"\"\"\n",
        "        Generate melody for an entire song by processing it in segments with proper timing\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        with open(vocab_path, 'r') as f:\n",
        "            vocabularies = json.load(f)\n",
        "\n",
        "        chord_to_idx = vocabularies['chord_vocab']\n",
        "        idx_to_note = vocabularies['idx_to_note']\n",
        "\n",
        "        chord_indices = []\n",
        "        for chord in full_chord_sequence:\n",
        "            chord_idx = chord_to_idx.get(chord, chord_to_idx.get('<UNK>', 0))\n",
        "            chord_indices.append(chord_idx)\n",
        "\n",
        "        if len(chord_indices) > self.max_chord_length:\n",
        "            print(f\"Warning: Chord sequence too long ({len(chord_indices)} > {self.max_chord_length}), truncating\")\n",
        "            chord_indices = chord_indices[:self.max_chord_length]\n",
        "\n",
        "        chord_times = []\n",
        "        if original_midi_path:\n",
        "            from utils import extract_chord_timing_from_midi\n",
        "            chord_times = extract_chord_timing_from_midi(original_midi_path)\n",
        "\n",
        "        original_length = len(chord_indices)\n",
        "        padded_chords = chord_indices + [chord_to_idx.get('<PAD>', 0)] * (self.max_chord_length - len(chord_indices))\n",
        "\n",
        "        chord_mask = [True] * original_length + [False] * (self.max_chord_length - original_length)\n",
        "\n",
        "        full_melody = []\n",
        "        step_size = max(1, segment_length - overlap)\n",
        "\n",
        "        print(f\"Generating melody for {len(chord_indices)} chords...\")\n",
        "        print(f\"Segment length: {segment_length}, Overlap: {overlap}, Step size: {step_size}\")\n",
        "        print(f\"Target density: {target_density:.2f}, Temperature: {temperature:.2f}\")\n",
        "\n",
        "        if chord_times and len(chord_times) >= len(chord_indices):\n",
        "            print(\"Using real chord timing from original MIDI\")\n",
        "            total_song_duration = chord_times[-1][1] - chord_times[0][0]\n",
        "            segment_durations = []\n",
        "            for i in range(0, len(chord_indices), step_size):\n",
        "                seg_start_idx = i\n",
        "                seg_end_idx = min(i + segment_length, len(chord_indices))\n",
        "\n",
        "                if seg_start_idx < len(chord_times) and seg_end_idx <= len(chord_times):\n",
        "                    seg_start_time = chord_times[seg_start_idx][0]\n",
        "                    seg_end_time = chord_times[seg_end_idx-1][1]\n",
        "                    segment_durations.append((seg_start_time, seg_end_time - seg_start_time))\n",
        "                else:\n",
        "                    estimated_duration = segment_length * 2.0\n",
        "                    estimated_start = i * 2.0\n",
        "                    segment_durations.append((estimated_start, estimated_duration))\n",
        "        else:\n",
        "            print(\"Using estimated chord timing (2 seconds per chord)\")\n",
        "            total_song_duration = len(chord_indices) * 2.0\n",
        "            segment_durations = []\n",
        "            for i in range(0, len(chord_indices), step_size):\n",
        "                estimated_start = i * 2.0\n",
        "                estimated_duration = min(segment_length, len(chord_indices) - i) * 2.0\n",
        "                segment_durations.append((estimated_start, estimated_duration))\n",
        "\n",
        "        current_position = 0\n",
        "        segment_idx = 0\n",
        "\n",
        "        print(f\"Total song duration: {total_song_duration:.1f}s\")\n",
        "        print(f\"Will generate segments until position {original_length}\")\n",
        "\n",
        "        while current_position < original_length:\n",
        "            segment_start = current_position\n",
        "            segment_end = min(segment_start + segment_length, original_length)\n",
        "            focus_position = (segment_start + segment_end) // 2\n",
        "            focus_position = min(focus_position, original_length - 1)\n",
        "\n",
        "            if segment_idx < len(segment_durations):\n",
        "                segment_start_time, segment_duration = segment_durations[segment_idx]\n",
        "            else:\n",
        "                segment_start_time = current_position * 2.0\n",
        "                remaining_chords = original_length - current_position\n",
        "                segment_duration = min(segment_length, remaining_chords) * 2.0\n",
        "\n",
        "            chord_tensor = torch.tensor([padded_chords], dtype=torch.long, device=device)\n",
        "            mask_tensor = torch.tensor([chord_mask], dtype=torch.bool, device=device)\n",
        "            focus_tensor = torch.tensor([focus_position], dtype=torch.long, device=device)\n",
        "            segment_pos = torch.tensor([current_position / max(1, original_length - 1)], dtype=torch.float32, device=device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                generated_sequence = self._generate_with_density_control(\n",
        "                    chord_tensor,\n",
        "                    mask_tensor,\n",
        "                    focus_tensor,\n",
        "                    target_density=target_density,\n",
        "                    density_window=density_window,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "\n",
        "            segment_melody = []\n",
        "            valid_notes = [note for note in generated_sequence if note['pitch'].item() > 0]\n",
        "\n",
        "            if valid_notes:\n",
        "                for i, note_info in enumerate(valid_notes):\n",
        "                    pitch_idx = note_info['pitch'].item()\n",
        "                    pitch = int(idx_to_note.get(str(pitch_idx), 60))\n",
        "\n",
        "                    note_position = i / len(valid_notes)\n",
        "                    relative_start = note_info['start'].item() / 8.0\n",
        "\n",
        "                    blended_start = 0.7 * note_position + 0.3 * relative_start\n",
        "\n",
        "                    actual_start = segment_start_time + (blended_start * segment_duration)\n",
        "\n",
        "                    relative_duration = min(1.0, note_info['duration'].item() / 4.0)\n",
        "                    actual_duration = max(0.1, min(1.0, relative_duration * segment_duration * 0.2))\n",
        "\n",
        "                    segment_melody.append({\n",
        "                        'pitch': pitch,\n",
        "                        'duration': actual_duration,\n",
        "                        'start_time': actual_start\n",
        "                    })\n",
        "\n",
        "            if segment_idx == 0:\n",
        "                full_melody.extend(segment_melody)\n",
        "            else:\n",
        "                overlap_end_time = segment_start_time + (overlap * segment_duration / segment_length)\n",
        "\n",
        "                for note in segment_melody:\n",
        "                    if note['start_time'] < overlap_end_time:\n",
        "                        if len(full_melody) == 0 or note['start_time'] > full_melody[-1]['start_time'] + 0.3:\n",
        "                            full_melody.append(note)\n",
        "                    else:\n",
        "                        full_melody.append(note)\n",
        "\n",
        "            print(f\"Generated segment {segment_idx + 1} \"\n",
        "                f\"(position: {current_position}-{segment_end}, focus: chord {focus_position}, \"\n",
        "                f\"time: {segment_start_time:.1f}-{segment_start_time+segment_duration:.1f}s, \"\n",
        "                f\"notes: {len(segment_melody)})\")\n",
        "\n",
        "            current_position += step_size\n",
        "            segment_idx += 1\n",
        "\n",
        "            if segment_idx > 200:\n",
        "                print(\"Warning: Too many segments generated, stopping\")\n",
        "                break\n",
        "\n",
        "        full_melody.sort(key=lambda x: x['start_time'])\n",
        "\n",
        "        if full_melody:\n",
        "            max_allowed_time = total_song_duration * 1.1\n",
        "            full_melody = [note for note in full_melody if note['start_time'] <= max_allowed_time]\n",
        "\n",
        "            if full_melody and full_melody[-1]['start_time'] + full_melody[-1]['duration'] > max_allowed_time:\n",
        "                full_melody[-1]['duration'] = max(0.1, max_allowed_time - full_melody[-1]['start_time'])\n",
        "\n",
        "        print(f\"Generated {len(full_melody)} notes for full song\")\n",
        "        if full_melody:\n",
        "            generated_duration = max(note['start_time'] + note['duration'] for note in full_melody)\n",
        "            print(f\"Generated melody duration: {generated_duration:.1f} seconds\")\n",
        "            print(f\"Expected song duration: {total_song_duration:.1f} seconds\")\n",
        "            coverage_ratio = generated_duration / total_song_duration if total_song_duration > 0 else 0\n",
        "            print(f\"Coverage ratio: {coverage_ratio:.2f}\")\n",
        "\n",
        "        return full_melody\n",
        "\n",
        "    def generate_melody_from_chords(self, chord_sequence, vocab_path, target_density=0.6,\n",
        "                                density_window=8, temperature=1.0, chord_duration=2.0):\n",
        "        \"\"\"Generate melody for a single chord sequence with proper timing\"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        original_device = next(self.parameters()).device\n",
        "        self.to('cpu')\n",
        "\n",
        "        with open(vocab_path, 'r') as f:\n",
        "            vocabularies = json.load(f)\n",
        "\n",
        "        chord_to_idx = vocabularies['chord_vocab']\n",
        "        idx_to_note = vocabularies['idx_to_note']\n",
        "\n",
        "        chord_indices = []\n",
        "        for chord in chord_sequence:\n",
        "            chord_idx = chord_to_idx.get(chord, chord_to_idx.get('<UNK>', 0))\n",
        "            chord_indices.append(chord_idx)\n",
        "\n",
        "        original_length = len(chord_indices)\n",
        "        while len(chord_indices) < self.max_chord_length:\n",
        "            chord_indices.append(chord_to_idx.get('<PAD>', 0))\n",
        "        chord_indices = chord_indices[:self.max_chord_length]\n",
        "\n",
        "        chord_tensor = torch.tensor([chord_indices], dtype=torch.long)\n",
        "        chord_mask = torch.tensor([[True] * original_length + [False] * (self.max_chord_length - original_length)],\n",
        "                                dtype=torch.bool)\n",
        "        focus_position = torch.tensor([original_length // 2], dtype=torch.long)\n",
        "\n",
        "        total_duration = len(chord_sequence) * chord_duration\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_sequence = self._generate_with_density_control(\n",
        "                chord_tensor, chord_mask, focus_position,\n",
        "                target_density, density_window, temperature\n",
        "            )\n",
        "\n",
        "        melody = []\n",
        "        for i, note_info in enumerate(generated_sequence):\n",
        "            pitch_idx = note_info['pitch'].item()\n",
        "            if pitch_idx > 0:\n",
        "                pitch = int(idx_to_note.get(str(pitch_idx), 60))\n",
        "\n",
        "                relative_start = note_info['start'].item()\n",
        "                relative_duration = note_info['duration'].item()\n",
        "\n",
        "                normalized_start = min(1.0, relative_start / 8.0)\n",
        "                normalized_duration = min(1.0, relative_duration / 4.0)\n",
        "\n",
        "                actual_start = normalized_start * total_duration\n",
        "                actual_duration = max(0.1, normalized_duration * total_duration * 0.1)\n",
        "\n",
        "                melody.append({\n",
        "                    'pitch': pitch,\n",
        "                    'duration': actual_duration,\n",
        "                    'start_time': actual_start\n",
        "                })\n",
        "\n",
        "        self.to(original_device)\n",
        "\n",
        "        return melody\n",
        "\n",
        "\n",
        "    def _generate_with_density_control(self, chord_tensor, chord_mask, focus_position,\n",
        "                             target_density=0.6, density_window=8, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate sequence with balanced density control\n",
        "        \"\"\"\n",
        "        batch_size = chord_tensor.size(0)\n",
        "        device = chord_tensor.device\n",
        "\n",
        "        chord_embedded = self.chord_embedding(chord_tensor)\n",
        "        chord_seq_len = chord_tensor.size(1)\n",
        "\n",
        "        chord_positions = torch.arange(chord_seq_len, device=device)\n",
        "        chord_pos_emb = self.chord_position_embedding(chord_positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        chord_embedded = chord_embedded + chord_pos_emb\n",
        "        chord_embedded = self.dropout(chord_embedded)\n",
        "\n",
        "        padding_mask = ~chord_mask\n",
        "\n",
        "        chord_encoded = self.chord_encoder(chord_embedded, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        focus_context, _ = self.focus_attention(chord_encoded, chord_encoded, chord_encoded, key_padding_mask=padding_mask)\n",
        "        chord_encoded = chord_encoded + self.focus_projection(focus_context)\n",
        "        chord_encoded = self.layer_norm(chord_encoded)\n",
        "\n",
        "        generated_sequence = []\n",
        "        current_input = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
        "\n",
        "        recent_predictions = []\n",
        "\n",
        "        print(f\"Generating with target density: {target_density:.2f}\")\n",
        "\n",
        "        for i in range(self.max_melody_length):\n",
        "            melody_embedded = self.note_embedding(current_input)\n",
        "            melody_seq_len = current_input.size(1)\n",
        "\n",
        "            melody_positions = torch.arange(melody_seq_len, device=device)\n",
        "            melody_pos_emb = self.position_embedding(melody_positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            melody_embedded = melody_embedded + melody_pos_emb\n",
        "            melody_embedded = self.dropout(melody_embedded)\n",
        "\n",
        "            decoded = self.melody_decoder(melody_embedded, chord_encoded, memory_key_padding_mask=padding_mask)\n",
        "\n",
        "            last_output = decoded[:, -1:]\n",
        "            pitch_logits = self.pitch_head(last_output)\n",
        "            duration_pred = self.duration_head(last_output).squeeze(-1)\n",
        "            start_pred = self.start_head(last_output).squeeze(-1)\n",
        "\n",
        "            if len(recent_predictions) >= density_window:\n",
        "                recent_notes = sum(1 for x in recent_predictions[-density_window:] if x > 0)\n",
        "                current_density = recent_notes / density_window\n",
        "\n",
        "                density_deviation = current_density - target_density\n",
        "\n",
        "                density_bias = self._calculate_density_bias(density_deviation, temperature)\n",
        "\n",
        "                if len(recent_predictions) >= 3:\n",
        "                    recent_note_count = sum(1 for x in recent_predictions[-3:] if x > 0)\n",
        "                    if recent_note_count >= 3:\n",
        "                        temporal_bias = -1.0\n",
        "                    elif recent_note_count == 0:\n",
        "                        temporal_bias = 0.5\n",
        "                    else:\n",
        "                        temporal_bias = 0.0\n",
        "                else:\n",
        "                    temporal_bias = 0.0\n",
        "\n",
        "                total_bias = density_bias + temporal_bias\n",
        "                pitch_logits = self._apply_density_bias(pitch_logits, total_bias)\n",
        "\n",
        "                if i % 8 == 0:\n",
        "                    print(f\"Step {i}: Density: {current_density:.3f}/{target_density:.3f}, \"\n",
        "                        f\"Bias: {total_bias:.3f}\")\n",
        "\n",
        "            if temperature != 1.0:\n",
        "                pitch_logits = pitch_logits / temperature\n",
        "\n",
        "            pitch_probs = torch.softmax(pitch_logits.squeeze(1), dim=-1)\n",
        "            next_pitch = torch.multinomial(pitch_probs, 1)\n",
        "\n",
        "            recent_predictions.append(next_pitch.item())\n",
        "\n",
        "            if len(recent_predictions) > density_window * 2:\n",
        "                recent_predictions = recent_predictions[-density_window:]\n",
        "\n",
        "            current_input = torch.cat([current_input, next_pitch], dim=1)\n",
        "\n",
        "            generated_sequence.append({\n",
        "                'pitch': next_pitch.squeeze(-1),\n",
        "                'duration': duration_pred,\n",
        "                'start': start_pred\n",
        "            })\n",
        "\n",
        "        return generated_sequence\n",
        "\n",
        "    def _calculate_density_bias(self, density_deviation, temperature):\n",
        "        \"\"\"\n",
        "        Calculate bias to apply to pitch logits based on density deviation\n",
        "        \"\"\"\n",
        "        bias_strength = 2.0 / temperature\n",
        "\n",
        "        if density_deviation > 0.15:\n",
        "            return -bias_strength\n",
        "        elif density_deviation < -0.15:\n",
        "            return bias_strength\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def _apply_density_bias(self, pitch_logits, bias):\n",
        "        \"\"\"\n",
        "        Apply density bias to pitch logits\n",
        "        \"\"\"\n",
        "        if bias == 0.0:\n",
        "            return pitch_logits\n",
        "\n",
        "        adjusted_logits = pitch_logits.clone()\n",
        "\n",
        "        if bias > 0:\n",
        "            adjusted_logits[:, :, 1:] += bias\n",
        "        else:\n",
        "            adjusted_logits[:, :, 0] += abs(bias)\n",
        "\n",
        "        return adjusted_logits\n",
        "\n",
        "    def generate_chord_aligned_melody(self, chord_sequence, chord_times, vocab_path, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate melody that aligns with real song timing by processing chord-by-chord\n",
        "        ENHANCED: Now uses timing enhancement methods\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        with open(vocab_path, 'r') as f:\n",
        "            vocabularies = json.load(f)\n",
        "\n",
        "        chord_to_idx = vocabularies['chord_vocab']\n",
        "        idx_to_note = vocabularies['idx_to_note']\n",
        "\n",
        "        chord_indices = []\n",
        "        for chord in chord_sequence:\n",
        "            chord_idx = chord_to_idx.get(chord, chord_to_idx.get('<UNK>', 0))\n",
        "            chord_indices.append(chord_idx)\n",
        "\n",
        "        if len(chord_indices) > self.max_chord_length:\n",
        "            print(f\"Warning: Chord sequence too long, using sliding window approach\")\n",
        "            return self._generate_with_sliding_window(chord_sequence, chord_times, vocab_path, temperature)\n",
        "\n",
        "        song_bpm = self._calculate_song_bpm(chord_times)\n",
        "        beat_duration = 60.0 / (song_bpm * 4)\n",
        "\n",
        "        padded_chords = chord_indices + [chord_to_idx.get('<PAD>', 0)] * (self.max_chord_length - len(chord_indices))\n",
        "        chord_mask = [True] * len(chord_indices) + [False] * (self.max_chord_length - len(chord_indices))\n",
        "\n",
        "        chord_durations = [end - start for start, end in chord_times[:len(chord_indices)]]\n",
        "        max_duration = max(chord_durations) if chord_durations else 4.0\n",
        "        normalized_durations = [d / max_duration for d in chord_durations]\n",
        "        normalized_durations += [0.0] * (self.max_chord_length - len(normalized_durations))\n",
        "\n",
        "        full_melody = []\n",
        "\n",
        "        print(f\"Generating melody for {len(chord_indices)} chords...\")\n",
        "\n",
        "        for focus_idx in range(len(chord_indices)):\n",
        "            chord_start, chord_end = chord_times[focus_idx]\n",
        "            chord_duration = chord_end - chord_start\n",
        "            current_chord = chord_sequence[focus_idx]\n",
        "\n",
        "            print(f\"  Chord {focus_idx + 1}/{len(chord_indices)}: {current_chord} ({chord_duration:.2f}s)\")\n",
        "\n",
        "            chord_tensor = torch.tensor([padded_chords], dtype=torch.long, device=device)\n",
        "            chord_duration_tensor = torch.tensor([normalized_durations], dtype=torch.float32, device=device)\n",
        "            chord_mask_tensor = torch.tensor([chord_mask], dtype=torch.bool, device=device)\n",
        "            focus_tensor = torch.tensor([focus_idx], dtype=torch.long, device=device)\n",
        "            target_duration_tensor = torch.tensor([chord_duration], dtype=torch.float32, device=device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                chord_melody = self._generate_single_chord_melody(\n",
        "                    chord_tensor,\n",
        "                    chord_duration_tensor,\n",
        "                    chord_mask_tensor,\n",
        "                    focus_tensor,\n",
        "                    target_duration_tensor,\n",
        "                    temperature\n",
        "                )\n",
        "\n",
        "            for note_info in chord_melody:\n",
        "                pitch_idx = note_info['pitch'].item()\n",
        "                if pitch_idx > 0:\n",
        "                    pitch = int(idx_to_note.get(str(pitch_idx), 60))\n",
        "\n",
        "                    relative_start = note_info['start'].item()\n",
        "                    relative_duration = note_info['duration'].item()\n",
        "\n",
        "                    absolute_start = chord_start + (relative_start * chord_duration)\n",
        "                    absolute_duration = max(0.1, relative_duration * chord_duration)\n",
        "\n",
        "                    if absolute_start + absolute_duration > chord_end:\n",
        "                        absolute_duration = chord_end - absolute_start\n",
        "\n",
        "                    if absolute_duration > 0.05:\n",
        "                        full_melody.append({\n",
        "                            'pitch': pitch,\n",
        "                            'start_time': absolute_start,\n",
        "                            'duration': absolute_duration,\n",
        "                            'chord': current_chord,\n",
        "                            'chord_index': focus_idx\n",
        "                        })\n",
        "\n",
        "        full_melody.sort(key=lambda x: x['start_time'])\n",
        "\n",
        "        cleaned_melody = self._enhance_note_timing_with_bpm(full_melody, beat_duration)\n",
        "\n",
        "        print(f\"Generated {len(cleaned_melody)} notes total\")\n",
        "        return cleaned_melody\n",
        "\n",
        "    def _generate_single_chord_melody(self, chord_tensor, chord_durations, chord_mask, focus_position, target_duration, temperature):\n",
        "        \"\"\"Generate melody for a single chord using the trained model\"\"\"\n",
        "\n",
        "        batch_size = chord_tensor.size(0)\n",
        "        device = chord_tensor.device\n",
        "\n",
        "        chord_embedded = self.chord_embedding(chord_tensor)\n",
        "        chord_seq_len = chord_tensor.size(1)\n",
        "\n",
        "        chord_positions = torch.arange(chord_seq_len, device=device)\n",
        "        chord_pos_emb = self.chord_position_embedding(chord_positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        chord_embedded = chord_embedded + chord_pos_emb\n",
        "        chord_embedded = self.dropout(chord_embedded)\n",
        "\n",
        "        padding_mask = ~chord_mask\n",
        "        chord_encoded = self.chord_encoder(chord_embedded, src_key_padding_mask=padding_mask)\n",
        "\n",
        "        focus_context, _ = self.focus_attention(chord_encoded, chord_encoded, chord_encoded, key_padding_mask=padding_mask)\n",
        "        chord_encoded = chord_encoded + self.focus_projection(focus_context)\n",
        "        chord_encoded = self.layer_norm(chord_encoded)\n",
        "\n",
        "        generated_sequence = []\n",
        "        current_input = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
        "\n",
        "        max_notes_per_chord = 8\n",
        "\n",
        "        for i in range(max_notes_per_chord):\n",
        "            melody_embedded = self.note_embedding(current_input)\n",
        "            melody_seq_len = current_input.size(1)\n",
        "\n",
        "            melody_positions = torch.arange(melody_seq_len, device=device)\n",
        "            melody_pos_emb = self.position_embedding(melody_positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            melody_embedded = melody_embedded + melody_pos_emb\n",
        "            melody_embedded = self.dropout(melody_embedded)\n",
        "\n",
        "            decoded = self.melody_decoder(melody_embedded, chord_encoded, memory_key_padding_mask=padding_mask)\n",
        "\n",
        "            last_output = decoded[:, -1:]\n",
        "            pitch_logits = self.pitch_head(last_output)\n",
        "            duration_pred = self.duration_head(last_output).squeeze(-1)\n",
        "            start_pred = self.start_head(last_output).squeeze(-1)\n",
        "\n",
        "            if temperature != 1.0:\n",
        "                pitch_logits = pitch_logits / temperature\n",
        "\n",
        "            pitch_probs = torch.softmax(pitch_logits.squeeze(1), dim=-1)\n",
        "            next_pitch = torch.multinomial(pitch_probs, 1)\n",
        "\n",
        "            current_input = torch.cat([current_input, next_pitch], dim=1)\n",
        "\n",
        "            generated_sequence.append({\n",
        "                'pitch': next_pitch.squeeze(-1),\n",
        "                'duration': torch.clamp(duration_pred, 0.0, 1.0),\n",
        "                'start': torch.clamp(start_pred, 0.0, 1.0)\n",
        "            })\n",
        "\n",
        "            if next_pitch.item() == 0:\n",
        "                break\n",
        "\n",
        "        return generated_sequence\n",
        "\n",
        "    def _remove_overlapping_notes(self, melody):\n",
        "        \"\"\"Remove overlapping notes to create clean melody line\"\"\"\n",
        "        if len(melody) <= 1:\n",
        "            return melody\n",
        "\n",
        "        cleaned = [melody[0]]\n",
        "\n",
        "        for note in melody[1:]:\n",
        "            last_note = cleaned[-1]\n",
        "\n",
        "            if note['start_time'] < last_note['start_time'] + last_note['duration']:\n",
        "                if last_note['start_time'] + 0.1 < note['start_time']:\n",
        "                    last_note['duration'] = note['start_time'] - last_note['start_time']\n",
        "                    cleaned.append(note)\n",
        "            else:\n",
        "                cleaned.append(note)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    def _generate_with_sliding_window(self, chord_sequence, chord_times, vocab_path, temperature):\n",
        "        \"\"\"Handle very long chord sequences with sliding window approach\"\"\"\n",
        "        print(f\"Using sliding window for {len(chord_sequence)} chords\")\n",
        "\n",
        "        window_size = self.max_chord_length - 2\n",
        "        overlap = window_size // 3\n",
        "        full_melody = []\n",
        "\n",
        "        num_windows = (len(chord_sequence) + window_size - overlap - 1) // (window_size - overlap)\n",
        "        print(f\"Processing {num_windows} windows (window_size={window_size}, overlap={overlap})\")\n",
        "\n",
        "        for window_idx in range(num_windows):\n",
        "            start_idx = window_idx * (window_size - overlap)\n",
        "            end_idx = min(start_idx + window_size, len(chord_sequence))\n",
        "\n",
        "            print(f\"  Window {window_idx + 1}/{num_windows}: chords {start_idx}-{end_idx} ({end_idx - start_idx} chords)\")\n",
        "\n",
        "            window_chords = chord_sequence[start_idx:end_idx]\n",
        "            window_times = chord_times[start_idx:end_idx]\n",
        "\n",
        "            if window_times:\n",
        "                time_offset = window_times[0][0]\n",
        "                adjusted_times = [(start - time_offset, end - time_offset) for start, end in window_times]\n",
        "            else:\n",
        "                adjusted_times = []\n",
        "\n",
        "            if len(window_chords) <= self.max_chord_length:\n",
        "                window_melody = self._generate_window_melody(\n",
        "                    window_chords,\n",
        "                    adjusted_times,\n",
        "                    vocab_path,\n",
        "                    temperature,\n",
        "                    time_offset\n",
        "                )\n",
        "            else:\n",
        "                window_chords = window_chords[:self.max_chord_length]\n",
        "                adjusted_times = adjusted_times[:self.max_chord_length]\n",
        "                window_melody = self._generate_window_melody(\n",
        "                    window_chords,\n",
        "                    adjusted_times,\n",
        "                    vocab_path,\n",
        "                    temperature,\n",
        "                    time_offset\n",
        "                )\n",
        "\n",
        "            if window_idx == 0:\n",
        "                full_melody.extend(window_melody)\n",
        "                print(f\"    Added {len(window_melody)} notes from first window\")\n",
        "            else:\n",
        "                overlap_chord_count = overlap\n",
        "                if start_idx + overlap_chord_count < len(chord_times):\n",
        "                    overlap_end_time = chord_times[start_idx + overlap_chord_count][0]\n",
        "                else:\n",
        "                    overlap_end_time = chord_times[-1][1]\n",
        "\n",
        "                new_notes = 0\n",
        "                for note in window_melody:\n",
        "                    if note['start_time'] >= overlap_end_time:\n",
        "                        full_melody.append(note)\n",
        "                        new_notes += 1\n",
        "\n",
        "                print(f\"    Added {new_notes} notes from window {window_idx + 1} (after overlap filtering)\")\n",
        "\n",
        "        print(f\"Sliding window complete: {len(full_melody)} total notes\")\n",
        "        return full_melody\n",
        "    def _generate_window_melody(self, chord_sequence, chord_times, vocab_path, temperature, time_offset):\n",
        "        \"\"\"\n",
        "        Generate melody for a single window\n",
        "        \"\"\"\n",
        "\n",
        "        with open(vocab_path, 'r') as f:\n",
        "            vocabularies = json.load(f)\n",
        "\n",
        "        chord_to_idx = vocabularies['chord_vocab']\n",
        "        idx_to_note = vocabularies['idx_to_note']\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        chord_indices = []\n",
        "        for chord in chord_sequence:\n",
        "            chord_idx = chord_to_idx.get(chord, chord_to_idx.get('<UNK>', 0))\n",
        "            chord_indices.append(chord_idx)\n",
        "\n",
        "        original_length = len(chord_indices)\n",
        "        padded_chords = chord_indices + [chord_to_idx.get('<PAD>', 0)] * (self.max_chord_length - len(chord_indices))\n",
        "        chord_mask = [True] * original_length + [False] * (self.max_chord_length - original_length)\n",
        "\n",
        "        chord_durations = [end - start for start, end in chord_times[:original_length]]\n",
        "        max_duration = max(chord_durations) if chord_durations else 4.0\n",
        "        normalized_durations = [d / max_duration for d in chord_durations]\n",
        "        normalized_durations += [0.0] * (self.max_chord_length - len(normalized_durations))\n",
        "\n",
        "        window_melody = []\n",
        "\n",
        "        for focus_idx in range(original_length):\n",
        "            if focus_idx >= len(chord_times):\n",
        "                break\n",
        "\n",
        "            chord_start, chord_end = chord_times[focus_idx]\n",
        "            chord_duration = chord_end - chord_start\n",
        "            current_chord = chord_sequence[focus_idx]\n",
        "\n",
        "            chord_tensor = torch.tensor([padded_chords], dtype=torch.long, device=device)\n",
        "            chord_duration_tensor = torch.tensor([normalized_durations], dtype=torch.float32, device=device)\n",
        "            chord_mask_tensor = torch.tensor([chord_mask], dtype=torch.bool, device=device)\n",
        "            focus_tensor = torch.tensor([focus_idx], dtype=torch.long, device=device)\n",
        "            target_duration_tensor = torch.tensor([chord_duration], dtype=torch.float32, device=device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                chord_melody = self._generate_single_chord_melody(\n",
        "                    chord_tensor,\n",
        "                    chord_duration_tensor,\n",
        "                    chord_mask_tensor,\n",
        "                    focus_tensor,\n",
        "                    target_duration_tensor,\n",
        "                    temperature\n",
        "                )\n",
        "\n",
        "            for note_info in chord_melody:\n",
        "                pitch_idx = note_info['pitch'].item()\n",
        "                if pitch_idx > 0:\n",
        "                    pitch = int(idx_to_note.get(str(pitch_idx), 60))\n",
        "\n",
        "                    relative_start = max(0.0, min(1.0, note_info['start'].item()))\n",
        "                    relative_duration = max(0.0, min(1.0, note_info['duration'].item()))\n",
        "\n",
        "                    absolute_start = time_offset + chord_start + (relative_start * chord_duration)\n",
        "                    absolute_duration = max(0.1, relative_duration * chord_duration * 0.5)\n",
        "\n",
        "                    chord_end_absolute = time_offset + chord_end\n",
        "                    if absolute_start + absolute_duration > chord_end_absolute:\n",
        "                        absolute_duration = max(0.1, chord_end_absolute - absolute_start)\n",
        "\n",
        "                    if absolute_duration > 0.05:\n",
        "                        window_melody.append({\n",
        "                            'pitch': pitch,\n",
        "                            'start_time': absolute_start,\n",
        "                            'duration': absolute_duration,\n",
        "                            'chord': current_chord,\n",
        "                            'chord_index': focus_idx\n",
        "                        })\n",
        "\n",
        "        enhanced_melody = self._enhance_note_timing(window_melody)\n",
        "\n",
        "        return enhanced_melody\n"
      ],
      "metadata": {
        "id": "bpYL2bCT_E6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "rwEcZGr4_GrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import time\n",
        "\n",
        "class ChordToMelodyTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "        self.pitch_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.duration_criterion = nn.MSELoss()\n",
        "        self.start_criterion = nn.MSELoss()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            patience=3,\n",
        "            factor=0.5,\n",
        "        )\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def calculate_accuracy(self, pitch_logits, target_pitch, mask):\n",
        "        with torch.no_grad():\n",
        "            pred_pitch = torch.argmax(pitch_logits, dim=-1)\n",
        "            target_flat = target_pitch.reshape(-1)\n",
        "            pred_flat = pred_pitch.reshape(-1)\n",
        "            mask_flat = mask.reshape(-1)\n",
        "\n",
        "            if mask_flat.sum() > 0:\n",
        "                correct = (pred_flat == target_flat) & mask_flat\n",
        "                accuracy = correct.sum().float() / mask_flat.sum().float()\n",
        "                return accuracy.item()\n",
        "            return 0.0\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_accuracy = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(self.train_loader):\n",
        "            full_chord_sequence = batch['full_chord_sequence'].to(self.device)\n",
        "            chord_durations = batch['chord_durations'].to(self.device)\n",
        "            chord_mask = batch['chord_mask'].to(self.device)\n",
        "            focus_positions = batch['focus_position'].to(self.device)\n",
        "            target_chord_duration = batch['target_chord_duration'].to(self.device)  # NEW\n",
        "            melody_pitch = batch['melody_pitch'].to(self.device)\n",
        "            melody_duration = batch['melody_duration'].to(self.device)\n",
        "            melody_start = batch['melody_start'].to(self.device)\n",
        "            melody_mask = batch['melody_mask'].to(self.device)\n",
        "\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            pitch_logits, duration_pred, start_pred = self.model(\n",
        "                full_chord_sequence,\n",
        "                chord_mask,\n",
        "                focus_positions,\n",
        "                chord_durations=chord_durations,\n",
        "                melody_pitch=melody_pitch,\n",
        "                training=True\n",
        "            )\n",
        "\n",
        "            target_pitch = melody_pitch[:, 1:]\n",
        "            target_duration = melody_duration[:, :-1]\n",
        "            target_start = melody_start[:, :-1]\n",
        "            target_mask = melody_mask[:, :-1]\n",
        "\n",
        "            pitch_loss = self.pitch_criterion(\n",
        "                pitch_logits.reshape(-1, pitch_logits.size(-1)),\n",
        "                target_pitch.reshape(-1)\n",
        "            )\n",
        "\n",
        "            if target_mask.sum() > 0:\n",
        "                duration_loss = self.duration_criterion(\n",
        "                    duration_pred[target_mask],\n",
        "                    target_duration[target_mask]\n",
        "                )\n",
        "                start_loss = self.start_criterion(\n",
        "                    start_pred[target_mask],\n",
        "                    target_start[target_mask]\n",
        "                )\n",
        "            else:\n",
        "                duration_loss = torch.tensor(0.0, device=self.device)\n",
        "                start_loss = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "            total_batch_loss = pitch_loss + 0.3 * duration_loss + 0.3 * start_loss\n",
        "\n",
        "            total_batch_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            accuracy = self.calculate_accuracy(pitch_logits, target_pitch, target_mask)\n",
        "\n",
        "            total_loss += total_batch_loss.item()\n",
        "            total_accuracy += accuracy\n",
        "            num_batches += 1\n",
        "\n",
        "            if batch_idx % 50 == 0 and batch_idx > 0:\n",
        "                current_loss = total_loss / (batch_idx + 1)\n",
        "                current_acc = total_accuracy / (batch_idx + 1)\n",
        "                print(f\"  Batch {batch_idx}/{len(self.train_loader)}: \"\n",
        "                    f\"Loss {current_loss:.4f}, Acc {current_acc:.3f}\")\n",
        "\n",
        "        return total_loss / num_batches, total_accuracy / num_batches\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_accuracy = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                full_chord_sequence = batch['full_chord_sequence'].to(self.device)\n",
        "                chord_durations = batch['chord_durations'].to(self.device)\n",
        "                chord_mask = batch['chord_mask'].to(self.device)\n",
        "                focus_positions = batch['focus_position'].to(self.device)\n",
        "                target_chord_duration = batch['target_chord_duration'].to(self.device)  # NEW\n",
        "                melody_pitch = batch['melody_pitch'].to(self.device)\n",
        "                melody_duration = batch['melody_duration'].to(self.device)\n",
        "                melody_start = batch['melody_start'].to(self.device)\n",
        "                melody_mask = batch['melody_mask'].to(self.device)\n",
        "\n",
        "                try:\n",
        "                    pitch_logits, duration_pred, start_pred = self.model(\n",
        "                        full_chord_sequence,\n",
        "                        chord_mask,\n",
        "                        focus_positions,\n",
        "                        chord_durations=chord_durations,\n",
        "                        melody_pitch=melody_pitch,\n",
        "                        training=True\n",
        "                    )\n",
        "                except RuntimeError as e:\n",
        "                    if \"MPS\" in str(e):\n",
        "                        self.model.cpu()\n",
        "                        pitch_logits, duration_pred, start_pred = self.model(\n",
        "                            full_chord_sequence.cpu(),\n",
        "                            chord_mask.cpu(),\n",
        "                            focus_positions.cpu(),\n",
        "                            chord_durations=chord_durations.cpu(),  # NEW\n",
        "                            melody_pitch=melody_pitch.cpu(),\n",
        "                            training=True\n",
        "                        )\n",
        "                        self.model.to(self.device)\n",
        "                        pitch_logits = pitch_logits.to(self.device)\n",
        "                        duration_pred = duration_pred.to(self.device)\n",
        "                        start_pred = start_pred.to(self.device)\n",
        "                    else:\n",
        "                        raise e\n",
        "\n",
        "                target_pitch = melody_pitch[:, 1:]\n",
        "                target_duration = melody_duration[:, :-1]\n",
        "                target_start = melody_start[:, :-1]\n",
        "                target_mask = melody_mask[:, :-1]\n",
        "\n",
        "                pitch_loss = self.pitch_criterion(\n",
        "                    pitch_logits.reshape(-1, pitch_logits.size(-1)),\n",
        "                    target_pitch.reshape(-1)\n",
        "                )\n",
        "\n",
        "                if target_mask.sum() > 0:\n",
        "                    duration_loss = self.duration_criterion(\n",
        "                        duration_pred[target_mask],\n",
        "                        target_duration[target_mask]\n",
        "                    )\n",
        "                    start_loss = self.start_criterion(\n",
        "                        start_pred[target_mask],\n",
        "                        target_start[target_mask]\n",
        "                    )\n",
        "                else:\n",
        "                    duration_loss = torch.tensor(0.0, device=self.device)\n",
        "                    start_loss = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "                total_batch_loss = pitch_loss + 0.3 * duration_loss + 0.3 * start_loss\n",
        "                accuracy = self.calculate_accuracy(pitch_logits, target_pitch, target_mask)\n",
        "\n",
        "                total_loss += total_batch_loss.item()\n",
        "                total_accuracy += accuracy\n",
        "                num_batches += 1\n",
        "\n",
        "        return total_loss / num_batches, total_accuracy / num_batches\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "            val_loss, val_acc = self.validate()\n",
        "\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.3f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.3f}\")\n",
        "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
        "            self.plot_training_history()\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), 'best_chord_melody_model.pt')\n",
        "                print(\"✓ New best model saved!\")\n",
        "\n",
        "            if epoch > 10 and val_loss > min(self.val_losses) * 1.2:\n",
        "                print(\"Early stopping - validation loss increasing\")\n",
        "                break\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = (end_time - start_time) / 60\n",
        "        print(f\"\\nTraining completed in {training_time:.1f} minutes\")\n",
        "        print(f\"Best validation loss: {self.best_val_loss:.4f}\")\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Training Loss', color='blue')\n",
        "        plt.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Progress')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if len(self.val_losses) > 3:\n",
        "            window = min(3, len(self.val_losses) // 2)\n",
        "            smoothed_val = []\n",
        "            for i in range(len(self.val_losses)):\n",
        "                start_idx = max(0, i - window)\n",
        "                end_idx = min(len(self.val_losses), i + window + 1)\n",
        "                smoothed_val.append(sum(self.val_losses[start_idx:end_idx]) / (end_idx - start_idx))\n",
        "\n",
        "            plt.plot(smoothed_val, label='Smoothed Val Loss', color='green')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Smoothed Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_history.png', dpi=150, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "BQweR4Fh_Mgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluator"
      ],
      "metadata": {
        "id": "7FG_WnVg_j95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class SimpleMelodyEvaluator:\n",
        "    def __init__(self):\n",
        "        # Simple chord-to-pitch mappings\n",
        "        self.chord_pitch_maps = {\n",
        "            'maj': [0, 4, 7],           # C major: C, E, G\n",
        "            'maj7': [0, 4, 7, 11],      # Cmaj7: C, E, G, B\n",
        "            'min': [0, 3, 7],           # C minor: C, Eb, G\n",
        "            'min7': [0, 3, 7, 10],      # Cm7: C, Eb, G, Bb\n",
        "            'minmaj7': [0, 3, 7, 11],   # CmMaj7: C, Eb, G, B\n",
        "            '7': [0, 4, 7, 10],         # C7: C, E, G, Bb\n",
        "            'dim': [0, 3, 6],           # Cdim: C, Eb, Gb\n",
        "            'aug': [0, 4, 8],           # Caug: C, E, G#\n",
        "            'sus2': [0, 2, 7],          # Csus2: C, D, G\n",
        "            'sus4': [0, 5, 7],          # Csus4: C, F, G\n",
        "        }\n",
        "\n",
        "    def parse_chord_symbol(self, chord_symbol: str) -> Tuple[int, str]:\n",
        "        \"\"\"Parse chord symbol into root note and quality\"\"\"\n",
        "        if ':' not in chord_symbol:\n",
        "            return 0, 'maj'\n",
        "\n",
        "        root_str, quality = chord_symbol.split(':', 1)\n",
        "\n",
        "        root_map = {\n",
        "            'C': 0, 'C#': 1, 'Db': 1, 'D': 2, 'D#': 3, 'Eb': 3,\n",
        "            'E': 4, 'F': 5, 'F#': 6, 'Gb': 6, 'G': 7, 'G#': 8,\n",
        "            'Ab': 8, 'A': 9, 'A#': 10, 'Bb': 10, 'B': 11\n",
        "        }\n",
        "\n",
        "        root = root_map.get(root_str, 0)\n",
        "        return root, quality\n",
        "\n",
        "    def get_chord_pitches(self, chord_symbol: str) -> List[int]:\n",
        "        \"\"\"Get pitch classes that belong to a chord\"\"\"\n",
        "        root, quality = self.parse_chord_symbol(chord_symbol)\n",
        "\n",
        "        if quality in self.chord_pitch_maps:\n",
        "            intervals = self.chord_pitch_maps[quality]\n",
        "        else:\n",
        "            intervals = self.chord_pitch_maps['maj']\n",
        "\n",
        "        return [(root + interval) % 12 for interval in intervals]\n",
        "\n",
        "    def find_chord_at_time(self, time: float, chord_times: List[Tuple]) -> int:\n",
        "        \"\"\"Find which chord is active at a given time\"\"\"\n",
        "        for i, chord_time in enumerate(chord_times):\n",
        "            if len(chord_time) >= 2:\n",
        "                start, end = chord_time[0], chord_time[1]\n",
        "                if start <= time < end:\n",
        "                    return i\n",
        "        return len(chord_times) - 1 if chord_times else 0\n",
        "\n",
        "    def chord_alignment_score(self, melody: List[Dict], chord_sequence: List[str], chord_times: List[Tuple]) -> float:\n",
        "        \"\"\"How well do melody notes fit the chords? (0.0 to 1.0)\"\"\"\n",
        "        if not melody or not chord_sequence:\n",
        "            return 0.0\n",
        "\n",
        "        alignment_scores = []\n",
        "\n",
        "        for note in melody:\n",
        "            chord_idx = self.find_chord_at_time(note['start_time'], chord_times)\n",
        "            if chord_idx < len(chord_sequence):\n",
        "                chord = chord_sequence[chord_idx]\n",
        "                chord_notes = self.get_chord_pitches(chord)\n",
        "                pitch_class = note['pitch'] % 12\n",
        "\n",
        "                if pitch_class in chord_notes:\n",
        "                    alignment_scores.append(1.0)\n",
        "                else:\n",
        "                    alignment_scores.append(0.0)\n",
        "            else:\n",
        "                alignment_scores.append(0.5)\n",
        "\n",
        "        return np.mean(alignment_scores) if alignment_scores else 0.0\n",
        "\n",
        "    def timing_accuracy_score(self, melody: List[Dict], expected_duration: float) -> float:\n",
        "        \"\"\"How well does generated melody match expected duration? (0.0 to 1.0)\"\"\"\n",
        "        if not melody or expected_duration <= 0:\n",
        "            return 0.0\n",
        "\n",
        "        actual_duration = max(n['start_time'] + n['duration'] for n in melody)\n",
        "\n",
        "        ratio = min(actual_duration, expected_duration) / max(actual_duration, expected_duration)\n",
        "        return ratio\n",
        "\n",
        "    def note_density_score(self, melody: List[Dict]) -> float:\n",
        "        \"\"\"How reasonable is the note density? (0.0 to 1.0)\"\"\"\n",
        "        if not melody:\n",
        "            return 0.0\n",
        "\n",
        "        total_duration = max(n['start_time'] + n['duration'] for n in melody)\n",
        "        if total_duration <= 0:\n",
        "            return 0.0\n",
        "\n",
        "        density = len(melody) / total_duration\n",
        "\n",
        "        if 0.5 <= density <= 2.0:\n",
        "            return 1.0\n",
        "        elif density < 0.5:\n",
        "            return density / 0.5\n",
        "        else:\n",
        "            return max(0.0, 1.0 - (density - 2.0) / 3.0)\n",
        "\n",
        "    def evaluate(self, melody: List[Dict], chord_sequence: List[str], chord_times: List[Tuple]) -> Dict[str, float]:\n",
        "        \"\"\"Main evaluation function - returns scores between 0.0 and 1.0\"\"\"\n",
        "\n",
        "        if chord_times and len(chord_times[0]) >= 2:\n",
        "            expected_duration = chord_times[-1][1]\n",
        "        else:\n",
        "            expected_duration = len(chord_sequence) * 2.0\n",
        "\n",
        "        scores = {\n",
        "            'chord_alignment': self.chord_alignment_score(melody, chord_sequence, chord_times),\n",
        "            'timing_accuracy': self.timing_accuracy_score(melody, expected_duration),\n",
        "            'note_density': self.note_density_score(melody)\n",
        "        }\n",
        "\n",
        "        scores['overall'] = (scores['chord_alignment'] + scores['timing_accuracy'] + scores['note_density']) / 3.0\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def print_evaluation(self, melody: List[Dict], chord_sequence: List[str], chord_times: List[Tuple]):\n",
        "        \"\"\"Print a nice evaluation report\"\"\"\n",
        "        scores = self.evaluate(melody, chord_sequence, chord_times)\n",
        "\n",
        "        print(f\"\\n🎵 MELODY EVALUATION\")\n",
        "        print(\"=\" * 30)\n",
        "        print(f\"Overall Score:     {scores['overall']:.3f}\")\n",
        "        print(f\"Chord Alignment:   {scores['chord_alignment']:.3f}\")\n",
        "        print(f\"Timing Accuracy:   {scores['timing_accuracy']:.3f}\")\n",
        "        print(f\"Note Density:      {scores['note_density']:.3f}\")\n",
        "\n",
        "        if scores['overall'] > 0.8:\n",
        "            print(\"🌟 Excellent!\")\n",
        "        elif scores['overall'] > 0.6:\n",
        "            print(\"✅ Good\")\n",
        "        elif scores['overall'] > 0.4:\n",
        "            print(\"⚠️  Okay\")\n",
        "        else:\n",
        "            print(\"❌ Needs work\")\n",
        "\n",
        "        if melody:\n",
        "            duration = max(n['start_time'] + n['duration'] for n in melody)\n",
        "            density = len(melody) / duration if duration > 0 else 0\n",
        "            pitch_range = max(n['pitch'] for n in melody) - min(n['pitch'] for n in melody)\n",
        "\n",
        "            print(f\"\\n📊 Stats:\")\n",
        "            print(f\"Notes: {len(melody)}\")\n",
        "            print(f\"Duration: {duration:.1f}s\")\n",
        "            print(f\"Density: {density:.2f} notes/sec\")\n",
        "            print(f\"Pitch range: {pitch_range} semitones\")\n",
        "\n",
        "        return scores\n",
        "\n",
        "\n",
        "def evaluate_melody(melody, chord_sequence, chord_times):\n",
        "    \"\"\"Simple function to evaluate a melody\"\"\"\n",
        "    evaluator = SimpleMelodyEvaluator()\n",
        "    return evaluator.print_evaluation(melody, chord_sequence, chord_times)"
      ],
      "metadata": {
        "id": "XuFcXNnk_mCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "lEaJDzzP_1Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Optional\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "\n",
        "def generate_midi_from_melody(melody: List[Dict], output_path: str, tempo: int = 120,\n",
        "                             original_midi_path: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Generate MIDI file from melody with proper timing\n",
        "    \"\"\"\n",
        "\n",
        "    original_tempo = tempo\n",
        "    if original_midi_path:\n",
        "        try:\n",
        "            original_midi = pretty_midi.PrettyMIDI(original_midi_path)\n",
        "            original_tempo = original_midi.estimate_tempo()\n",
        "            print(f\"Original tempo: {original_tempo:.1f} BPM\")\n",
        "        except:\n",
        "            print(f\"Could not load original MIDI, using default tempo: {tempo} BPM\")\n",
        "            original_tempo = tempo\n",
        "\n",
        "    midi = pretty_midi.PrettyMIDI(initial_tempo=original_tempo)\n",
        "\n",
        "    melody_instrument = pretty_midi.Instrument(program=1, name=\"Generated Melody\")\n",
        "\n",
        "    if not melody:\n",
        "        print(\"Warning: Empty melody provided\")\n",
        "        midi.instruments.append(melody_instrument)\n",
        "        midi.write(output_path)\n",
        "        return\n",
        "\n",
        "    for note_info in melody:\n",
        "        try:\n",
        "            pitch = int(note_info['pitch'])\n",
        "            start_time = float(note_info['start_time'])\n",
        "            duration = float(note_info['duration'])\n",
        "\n",
        "            duration = max(0.1, duration)\n",
        "            if not (21 <= pitch <= 108):\n",
        "                continue\n",
        "\n",
        "            note = pretty_midi.Note(\n",
        "                velocity=80,\n",
        "                pitch=pitch,\n",
        "                start=start_time,\n",
        "                end=start_time + duration\n",
        "            )\n",
        "            melody_instrument.notes.append(note)\n",
        "\n",
        "        except (KeyError, ValueError, TypeError) as e:\n",
        "            print(f\"Warning: Skipping invalid note {note_info}: {e}\")\n",
        "            continue\n",
        "\n",
        "    melody_instrument.notes.sort(key=lambda n: n.start)\n",
        "\n",
        "    midi.instruments.append(melody_instrument)\n",
        "\n",
        "    if original_midi_path:\n",
        "        try:\n",
        "            original_midi = pretty_midi.PrettyMIDI(original_midi_path)\n",
        "            for i, instrument in enumerate(original_midi.instruments):\n",
        "                if i > 0 or \"melody\" not in instrument.name.lower():\n",
        "                    instrument.name = f\"Original_{instrument.name}\"\n",
        "                    midi.instruments.append(instrument)\n",
        "            print(f\"Added {len(original_midi.instruments)-1} original tracks\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not add original tracks: {e}\")\n",
        "\n",
        "    midi.write(output_path)\n",
        "\n",
        "    if melody_instrument.notes:\n",
        "        total_duration = melody_instrument.notes[-1].end\n",
        "        note_count = len(melody_instrument.notes)\n",
        "        print(f\"Generated melody: {note_count} notes, {total_duration:.1f}s duration\")\n",
        "    else:\n",
        "        print(\"Warning: No valid notes generated\")\n",
        "\n",
        "def extract_chord_timing_from_midi(midi_path: str, target_chord_count: int = None) -> List[tuple]:\n",
        "    \"\"\"\n",
        "    Extract actual chord timing from original MIDI file with better alignment\n",
        "    \"\"\"\n",
        "    try:\n",
        "        midi = pretty_midi.PrettyMIDI(midi_path)\n",
        "        total_duration = midi.get_end_time()\n",
        "\n",
        "        print(f\"MIDI Analysis: {midi_path}\")\n",
        "        print(f\"  Total duration: {total_duration:.1f}s\")\n",
        "        print(f\"  Target chord count: {target_chord_count}\")\n",
        "\n",
        "        accompaniment_track = None\n",
        "        max_polyphony = 0\n",
        "\n",
        "        for i, instrument in enumerate(midi.instruments):\n",
        "            if instrument.is_drum:\n",
        "                continue\n",
        "\n",
        "            polyphony = calculate_average_polyphony(instrument)\n",
        "            print(f\"  Track {i} ({instrument.name}): {len(instrument.notes)} notes, polyphony: {polyphony:.1f}\")\n",
        "\n",
        "            if \"melody\" in instrument.name.lower() or \"lead\" in instrument.name.lower():\n",
        "                continue\n",
        "\n",
        "            if polyphony > max_polyphony:\n",
        "                max_polyphony = polyphony\n",
        "                accompaniment_track = instrument\n",
        "\n",
        "        if not accompaniment_track:\n",
        "            print(\"  No suitable accompaniment track found, using fallback\")\n",
        "            return create_fallback_timing(total_duration, target_chord_count)\n",
        "\n",
        "        print(f\"  Using track: {accompaniment_track.name} (polyphony: {max_polyphony:.1f})\")\n",
        "\n",
        "        chord_changes = detect_chord_changes(accompaniment_track, total_duration)\n",
        "\n",
        "        print(f\"  Detected {len(chord_changes)} chord changes\")\n",
        "\n",
        "        if target_chord_count and len(chord_changes) != target_chord_count:\n",
        "            chord_changes = align_chord_timing(chord_changes, target_chord_count, total_duration)\n",
        "            print(f\"  Aligned to {len(chord_changes)} chord segments\")\n",
        "\n",
        "        return chord_changes\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting chord timing: {e}\")\n",
        "        if target_chord_count:\n",
        "            return create_fallback_timing(60.0, target_chord_count)\n",
        "        return []\n",
        "\n",
        "def calculate_average_polyphony(instrument):\n",
        "    \"\"\"Calculate average number of simultaneous notes\"\"\"\n",
        "    if not instrument.notes:\n",
        "        return 0.0\n",
        "\n",
        "    duration = max(note.end for note in instrument.notes)\n",
        "    sample_points = int(duration * 4)\n",
        "\n",
        "    total_polyphony = 0\n",
        "    for i in range(sample_points):\n",
        "        time_point = i * 0.25\n",
        "        active_notes = sum(1 for note in instrument.notes\n",
        "                          if note.start <= time_point < note.end)\n",
        "        total_polyphony += active_notes\n",
        "\n",
        "    return total_polyphony / sample_points if sample_points > 0 else 0\n",
        "\n",
        "def detect_chord_changes(instrument, total_duration):\n",
        "    \"\"\"Detect when chords change in the instrument\"\"\"\n",
        "    if not instrument.notes:\n",
        "        return []\n",
        "\n",
        "    chord_events = []\n",
        "    tolerance = 0.15\n",
        "\n",
        "    sorted_notes = sorted(instrument.notes, key=lambda n: n.start)\n",
        "\n",
        "    i = 0\n",
        "    while i < len(sorted_notes):\n",
        "        chord_start = sorted_notes[i].start\n",
        "        chord_notes = [sorted_notes[i]]\n",
        "\n",
        "        j = i + 1\n",
        "        while j < len(sorted_notes) and sorted_notes[j].start - chord_start <= tolerance:\n",
        "            chord_notes.append(sorted_notes[j])\n",
        "            j += 1\n",
        "\n",
        "        if len(chord_notes) >= 2 or (j < len(sorted_notes) and sorted_notes[j].start - chord_start > 0.5):\n",
        "            chord_events.append({\n",
        "                'start': chord_start,\n",
        "                'notes': chord_notes,\n",
        "                'polyphony': len(chord_notes)\n",
        "            })\n",
        "\n",
        "        i = j\n",
        "\n",
        "    chord_times = []\n",
        "    for i, event in enumerate(chord_events):\n",
        "        start_time = event['start']\n",
        "\n",
        "        if i + 1 < len(chord_events):\n",
        "            end_time = chord_events[i + 1]['start']\n",
        "        else:\n",
        "            end_time = max(note.end for note in event['notes'])\n",
        "            end_time = min(end_time, total_duration)\n",
        "\n",
        "        if end_time - start_time >= 0.3:\n",
        "            chord_times.append((start_time, end_time, len(chord_times)))\n",
        "\n",
        "    return chord_times\n",
        "\n",
        "def align_chord_timing(detected_changes, target_count, total_duration):\n",
        "    \"\"\"Align detected chord changes to target chord count\"\"\"\n",
        "\n",
        "    if len(detected_changes) == target_count:\n",
        "        return detected_changes\n",
        "\n",
        "    print(f\"  Aligning {len(detected_changes)} detected changes to {target_count} target chords\")\n",
        "\n",
        "    if len(detected_changes) > target_count:\n",
        "        return merge_chord_segments(detected_changes, target_count)\n",
        "    else:\n",
        "        return interpolate_chord_segments(detected_changes, target_count, total_duration)\n",
        "        return interpolate_chord_segments(detected_changes, target_count, total_duration)\n",
        "\n",
        "def merge_chord_segments(segments, target_count):\n",
        "    \"\"\"Merge chord segments to reach target count\"\"\"\n",
        "    if target_count >= len(segments):\n",
        "        return segments\n",
        "\n",
        "    indices = [int(i * len(segments) / target_count) for i in range(target_count)]\n",
        "    merged = []\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        start_time = segments[idx][0]\n",
        "\n",
        "        if i + 1 < len(indices):\n",
        "            next_idx = indices[i + 1]\n",
        "            end_time = segments[next_idx][0]\n",
        "        else:\n",
        "            end_time = segments[-1][1]\n",
        "\n",
        "        merged.append((start_time, end_time, i))\n",
        "\n",
        "    return merged\n",
        "\n",
        "def interpolate_chord_segments(segments, target_count, total_duration):\n",
        "    \"\"\"Interpolate to create more chord segments\"\"\"\n",
        "    if not segments:\n",
        "        return create_fallback_timing(total_duration, target_count)\n",
        "\n",
        "    chord_duration = total_duration / target_count\n",
        "    interpolated = []\n",
        "\n",
        "    for i in range(target_count):\n",
        "        start_time = i * chord_duration\n",
        "        end_time = (i + 1) * chord_duration\n",
        "        interpolated.append((start_time, end_time, i))\n",
        "\n",
        "    return interpolated\n",
        "\n",
        "def create_fallback_timing(duration, chord_count):\n",
        "    \"\"\"Create uniform chord timing as fallback\"\"\"\n",
        "    if chord_count <= 0:\n",
        "        return []\n",
        "\n",
        "    chord_duration = duration / chord_count\n",
        "    return [(i * chord_duration, (i + 1) * chord_duration, i)\n",
        "            for i in range(chord_count)]\n",
        "\n",
        "\n",
        "def align_melody_to_chord_timing(melody: List[Dict], chord_times: List[tuple]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Align generated melody to actual chord timing from original MIDI\n",
        "    \"\"\"\n",
        "    if not melody or not chord_times:\n",
        "        return melody\n",
        "\n",
        "    total_chord_duration = chord_times[-1][1] - chord_times[0][0]\n",
        "\n",
        "    melody_start = min(note['start_time'] for note in melody)\n",
        "    melody_end = max(note['start_time'] + note.get('duration', 0) for note in melody)\n",
        "    melody_span = melody_end - melody_start\n",
        "\n",
        "    if melody_span <= 0:\n",
        "        return melody\n",
        "\n",
        "    time_scale = total_chord_duration / melody_span\n",
        "    time_offset = chord_times[0][0] - melody_start * time_scale\n",
        "\n",
        "    print(f\"Aligning melody: scale={time_scale:.3f}, offset={time_offset:.3f}s\")\n",
        "\n",
        "    aligned_melody = []\n",
        "    for note in melody:\n",
        "        aligned_note = note.copy()\n",
        "        aligned_note['start_time'] = note['start_time'] * time_scale + time_offset\n",
        "        aligned_note['duration'] = note.get('duration', 0.5) * time_scale\n",
        "\n",
        "        aligned_note['start_time'] = max(0, aligned_note['start_time'])\n",
        "        aligned_note['duration'] = max(0.1, min(4.0, aligned_note['duration']))\n",
        "\n",
        "        aligned_melody.append(aligned_note)\n",
        "\n",
        "    return aligned_melody"
      ],
      "metadata": {
        "id": "BtV8jHgwARQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Pipeline"
      ],
      "metadata": {
        "id": "2kmwYp7GAdiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from data_processor import process_dataset\n",
        "from dataset import ChordMelodyDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from model import AttentionChordToMelodyTransformer\n",
        "from trainer import ChordToMelodyTrainer\n",
        "from utils import generate_midi_from_melody\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import os\n",
        "import pretty_midi\n",
        "from evaluator import evaluate_melody, SimpleMelodyEvaluator\n",
        "\n",
        "def evaluate_generated_melody(melody, chord_sequence, chord_times):\n",
        "    return evaluate_melody(melody, chord_sequence, chord_times)\n",
        "\n",
        "def main(args):\n",
        "    print(\"Chord to Melody Generation - Attention-Based Global Context Approach\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Process dataset if needed\n",
        "    if args.process_data:\n",
        "        print(\"Processing dataset...\")\n",
        "        dataset_path, output_path = \"POP909-Dataset\", \"processed_pop909_chord_melody\"\n",
        "        process_dataset(dataset_path, output_path, melody_segment_length=32)\n",
        "        print(\"Dataset processing completed!\")\n",
        "        return\n",
        "\n",
        "    # Choose device - always use CPU for generation due to MPS limitations\n",
        "    training_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    generation_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {training_device} (training) / {generation_device} (generation)\")\n",
        "\n",
        "    # Load vocabularies & dataset\n",
        "    data_path = \"processed_pop909_chord_melody/training_sequences.pkl\"\n",
        "    vocab_path = \"processed_pop909_chord_melody/vocabularies.json\"\n",
        "\n",
        "    try:\n",
        "        full_dataset = ChordMelodyDataset(data_path, vocab_path, max_chord_length=12, max_melody_length=16)\n",
        "        print(f\"Dataset loaded successfully!\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Processed data not found. Please run with --process_data flag first.\")\n",
        "        return\n",
        "\n",
        "    # Split dataset into train / val\n",
        "    train_size = int(0.85 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    batch_size = 8 if training_device.type == 'cuda' else 16  # Smaller batch for MPS\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Create model\n",
        "    model = AttentionChordToMelodyTransformer(\n",
        "        chord_vocab_size=len(full_dataset.chord_to_idx),\n",
        "        note_vocab_size=len(full_dataset.note_to_idx),\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_layers=6,\n",
        "        max_chord_length=12\n",
        "    )\n",
        "\n",
        "    print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    if args.train:\n",
        "        # Move model to training device\n",
        "        model = model.to(training_device)\n",
        "        # Create trainer and train\n",
        "        trainer = ChordToMelodyTrainer(model, train_loader, val_loader, training_device)\n",
        "        trainer.train(num_epochs=args.epochs)\n",
        "        trainer.plot_training_history()\n",
        "        print(\"Training completed!\")\n",
        "    else:\n",
        "        # Load trained model\n",
        "        try:\n",
        "            model.load_state_dict(torch.load('best_chord_melody_model.pt', map_location=generation_device))\n",
        "            model = model.to(generation_device)\n",
        "            print(\"Loaded trained model successfully!\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"No trained model found. Please train first with --train flag.\")\n",
        "            return\n",
        "\n",
        "    # Move model to CPU for generation\n",
        "    model = model.to(generation_device)\n",
        "\n",
        "    # Generate sample melodies\n",
        "    print(\"\\nGENERATING SAMPLE MELODY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Test with real POP909 song\n",
        "    if args.test_real_song:\n",
        "        print(\"\\nReal POP909 song with chord-aligned timing:\")\n",
        "        try:\n",
        "            with open(\"processed_pop909_chord_melody/chord_melody_data.json\", 'r') as f:\n",
        "                song_data = json.load(f)\n",
        "\n",
        "            # ✅ Fixed: Only test a few songs instead of looping through all\n",
        "            test_indices = [i for i in range(len(song_data)-100, len(song_data))]\n",
        "\n",
        "            for song_idx in test_indices:\n",
        "                if song_idx >= len(song_data):\n",
        "                    continue\n",
        "\n",
        "                test_song = song_data[song_idx]\n",
        "                real_chords = test_song['full_chord_sequence']\n",
        "                song_id = test_song['song_id']\n",
        "\n",
        "                print(f\"\\n\" + \"=\"*50)\n",
        "                print(f\"TESTING SONG {song_idx + 1}: {song_id}\")\n",
        "                print(f\"=\"*50)\n",
        "\n",
        "                # Path to original MIDI file\n",
        "                original_midi_path = f\"POP909-Dataset/POP909/{song_id}/{song_id}.mid\"\n",
        "\n",
        "                print(f\"Chord progression: {len(real_chords)} chords\")\n",
        "                print(f\"Chords: {' | '.join(real_chords[:8])}{'...' if len(real_chords) > 8 else ''}\")\n",
        "\n",
        "                # DEBUG: Check original MIDI properties\n",
        "                original_tempo = 120  # Default\n",
        "                try:\n",
        "                    original_midi = pretty_midi.PrettyMIDI(original_midi_path)\n",
        "                    original_duration = original_midi.get_end_time()\n",
        "                    original_tempo = original_midi.estimate_tempo()\n",
        "\n",
        "                    print(f\"\\nOriginal MIDI Analysis:\")\n",
        "                    print(f\"  Duration: {original_duration:.1f}s\")\n",
        "                    print(f\"  Estimated tempo: {original_tempo:.1f} BPM\")\n",
        "                    print(f\"  Instruments: {len(original_midi.instruments)}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not analyze original MIDI: {e}\")\n",
        "                    original_duration = len(real_chords) * 2.0  # Fallback\n",
        "\n",
        "                # Extract chord timing from original MIDI\n",
        "                print(f\"\\nExtracting chord timing...\")\n",
        "                if original_midi_path and Path(original_midi_path).exists():\n",
        "                    from utils import extract_chord_timing_from_midi\n",
        "                    chord_times = extract_chord_timing_from_midi(\n",
        "                        original_midi_path,\n",
        "                        target_chord_count=len(real_chords)\n",
        "                    )\n",
        "                else:\n",
        "                    print(\"  Original MIDI not found, using estimated timing\")\n",
        "                    chord_times = [(i * 2.0, (i + 1) * 2.0, i) for i in range(len(real_chords))]\n",
        "\n",
        "                if chord_times:\n",
        "                    extracted_duration = chord_times[-1][1] - chord_times[0][0]\n",
        "                    print(f\"  Extracted {len(chord_times)} chord segments\")\n",
        "                    print(f\"  Extracted duration: {extracted_duration:.1f}s\")\n",
        "                    print(f\"  Average chord duration: {extracted_duration/len(chord_times):.1f}s\")\n",
        "                else:\n",
        "                    print(\"  Failed to extract timing, using fallback\")\n",
        "                    chord_times = [(i * 2.0, (i + 1) * 2.0, i) for i in range(len(real_chords))]\n",
        "\n",
        "                # Generate melody with chord-aligned timing\n",
        "                print(f\"\\nGenerating chord-aligned melody...\")\n",
        "\n",
        "                try:\n",
        "                    generated_melody_stored_path = f\"generated_melody_stored/{song_id}.json\"\n",
        "                    if os.path.exists(generated_melody_stored_path):\n",
        "                        with open(generated_melody_stored_path, 'r') as f:\n",
        "                            generated_melody = json.load(f)\n",
        "                    else:\n",
        "                        # Generate using the chord-aligned method\n",
        "                        generated_melody = model.generate_chord_aligned_melody(\n",
        "                            chord_sequence=real_chords,\n",
        "                            chord_times=[(start, end) for start, end, _ in chord_times],  # Remove index\n",
        "                            vocab_path=vocab_path,\n",
        "                            temperature=1.1\n",
        "                        )\n",
        "                        os.makedirs(\"generated_melody_stored\", exist_ok=True)\n",
        "                        with open(generated_melody_stored_path, 'w') as f:\n",
        "                            json.dump(generated_melody, f, indent=2)\n",
        "\n",
        "                    if generated_melody:\n",
        "                        print(f\"✓ Generated {len(generated_melody)} notes\")\n",
        "\n",
        "                        # Calculate generated duration\n",
        "                        generated_duration = max(note['start_time'] + note['duration'] for note in generated_melody)\n",
        "                        expected_duration = chord_times[-1][1] if chord_times else len(real_chords) * 2.0\n",
        "\n",
        "                        print(f\"  Generated duration: {generated_duration:.1f}s\")\n",
        "                        print(f\"  Expected duration: {expected_duration:.1f}s\")\n",
        "                        print(f\"  Coverage ratio: {generated_duration/expected_duration:.2f}\")\n",
        "\n",
        "                        evaluation_scores = evaluate_generated_melody(\n",
        "                            generated_melody,\n",
        "                            real_chords,\n",
        "                            [(start, end, 0) for start, end, _ in chord_times]\n",
        "                        )\n",
        "\n",
        "                        evaluator = SimpleMelodyEvaluator()\n",
        "                        print(f\"\\n🔍 CHORD-BY-CHORD ANALYSIS:\")\n",
        "                        for i, chord in enumerate(real_chords[:5]):\n",
        "                            if i >= len(chord_times):\n",
        "                                break\n",
        "\n",
        "                            chord_start, chord_end = chord_times[i][0], chord_times[i][1]\n",
        "\n",
        "                            chord_notes = [n for n in generated_melody\n",
        "                                          if chord_start <= n['start_time'] < chord_end]\n",
        "\n",
        "                            if chord_notes:\n",
        "                                chord_alignment = evaluator.chord_alignment_score(\n",
        "                                    chord_notes, [chord], [(chord_start, chord_end, 0)]\n",
        "                                )\n",
        "\n",
        "                                pitches = [n['pitch'] % 12 for n in chord_notes]\n",
        "                                print(f\"  Chord {i+1} ({chord}): {chord_alignment:.3f} \"\n",
        "                                      f\"| Notes: {len(chord_notes)} | Pitches: {pitches}\")\n",
        "                            else:\n",
        "                                print(f\"  Chord {i+1} ({chord}): No notes generated\")\n",
        "\n",
        "                        evaluation_results = {\n",
        "                            'song_id': song_id,\n",
        "                            'scores': evaluation_scores,\n",
        "                            'melody_stats': {\n",
        "                                'num_notes': len(generated_melody),\n",
        "                                'duration': max(n['start_time'] + n['duration'] for n in generated_melody),\n",
        "                                'pitch_range': max(n['pitch'] for n in generated_melody) - min(n['pitch'] for n in generated_melody),\n",
        "                                'avg_note_duration': sum(n['duration'] for n in generated_melody) / len(generated_melody)\n",
        "                            },\n",
        "                            'chord_progression': real_chords[:10],\n",
        "                            'timing_info': {\n",
        "                                'expected_duration': expected_duration,\n",
        "                                'generated_duration': generated_duration,\n",
        "                                'coverage_ratio': generated_duration / expected_duration if expected_duration > 0 else 0\n",
        "                            }\n",
        "                        }\n",
        "\n",
        "                        os.makedirs(\"evaluation_results\", exist_ok=True)\n",
        "                        with open(f'evaluation_results/evaluation_{song_id}.json', 'w') as f:\n",
        "                            json.dump(evaluation_results, f, indent=2)\n",
        "\n",
        "                        print(f\"✅ Evaluation results saved to evaluation_results/evaluation_{song_id}.json\")\n",
        "\n",
        "                        os.makedirs(\"generated_real_song_aligned\", exist_ok=True)\n",
        "                        output_path = f\"generated_real_song_aligned/{song_id}.mid\"\n",
        "\n",
        "                        try:\n",
        "                            generate_midi_from_melody(\n",
        "                                generated_melody,\n",
        "                                output_path,\n",
        "                                tempo=original_tempo,\n",
        "                                original_midi_path=original_midi_path\n",
        "                            )\n",
        "                            print(f\"✓ Saved MIDI to {output_path}\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"  Error saving MIDI: {e}\")\n",
        "                            simple_output = f\"generated_melody_only_{song_id}.mid\"\n",
        "                            generate_midi_from_melody(generated_melody, simple_output)\n",
        "                            print(f\"  Saved melody-only version to {simple_output}\")\n",
        "\n",
        "                        if 'original_duration' in locals():\n",
        "                            print(f\"\\nTiming Comparison:\")\n",
        "                            print(f\"  Original MIDI:     {original_duration:.1f}s\")\n",
        "                            print(f\"  Generated melody:  {generated_duration:.1f}s\")\n",
        "                            timing_accuracy = 1 - abs(generated_duration - original_duration)/original_duration\n",
        "                            print(f\"  Timing accuracy:   {timing_accuracy:.1%}\")\n",
        "\n",
        "                            if timing_accuracy > 0.9:\n",
        "                                print(f\"  ✅ Excellent timing alignment!\")\n",
        "                            elif timing_accuracy > 0.8:\n",
        "                                print(f\"  ✓ Good timing alignment\")\n",
        "                            else:\n",
        "                                print(f\"  ⚠️ Timing could be improved\")\n",
        "\n",
        "                    else:\n",
        "                        print(\"  ❌ No melody generated\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ❌ Generation failed: {e}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"Processed song data not found. Please run with --process_data flag first.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in real song test: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Attention-Based Chord to Melody Generation\")\n",
        "    parser.add_argument(\"--train\", action=\"store_true\", help=\"Train the model\")\n",
        "    parser.add_argument(\"--process_data\", action=\"store_true\", help=\"Process the dataset\")\n",
        "    parser.add_argument(\"--test_real_song\", action=\"store_true\", help=\"Test with real song chord progression\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of training epochs\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "E-vbW4-uAfH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 - Continuous Conditioned Generation"
      ],
      "metadata": {
        "id": "_Ih2nckKDuo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generaral Structure\n",
        "Note: I will only copy-paste important python codes here.\n",
        "It's not going to be executable, because the code depends on a lot of other stuff from the repository. Below is the repository URL. If you are interested, you can view complete implementation of this project.\n",
        "https://github.com/SelenaGeRuiqi/stable-audio-tools\n",
        "### Dataset Collection and Preprocessing\n",
        "\n",
        "|-- piano_collector.py ## See [piano_collector](piano_collector.py)\n",
        "|-- classical_piano_dataset\n",
        "|   |-- processed_10s # Store .wav files\n",
        "|   |-- classical_piano_prompts.json\n",
        "|   |-- custom_metadata.py\n",
        "|   |-- dataset_summary.json\n",
        "|   `-- dataset_config.json\n",
        "\n",
        "### Training\n",
        "\n",
        "|-- stable-audio-open-1.0\n",
        "|   |-- model.ckpt  ## Baseline checkpoint\n",
        "|   |-- model_config.json  # Use this for training and evaluation, see [model config](stable-audio-open-1.0/model_config.json)\n",
        "|   `-- ...\n",
        "|-- stable_audio_tools\n",
        "|   |-- models\n",
        "|   |   |-- diffusion.py\n",
        "|   |   |-- autoencoders.py\n",
        "|   |   |-- pretransforms.py\n",
        "|   |   |-- convnext.py\n",
        "|   |   |-- pretrained.py\n",
        "|   |   |-- conditioners.py\n",
        "|   |   `-- ...\n",
        "|   `-- ...\n",
        "|-- train.py  ## Train the whole model, see [train instruction](train_instruction.md)\n",
        "|-- train_freeze.py  ## Train the model with frozen T5 encoder and part of the unet, see [train_freeze](train_freeze.py)\n",
        "`-- wandb\n",
        "|-- checkpoints\n",
        "|   `-- piano_1000_clips\n",
        "\n",
        "### Music Generation(Samples will be shown at the end of the presentation)\n",
        "\n",
        "|-- unwrap_model.py  ## Unwrap the finetuned model to get the checkpoint\n",
        "|-- sao_piano_1000clips.ckpt  ## After unwrap\n",
        "|-- run_gradio.py  ## Run the gradio api to test the finetuned model and generate samples\n",
        "\n",
        "### Evaluation\n",
        "\n",
        "|-- evaluation_piano.py ## See [evaluation_piano](evaluation_piano.py)\n",
        "|-- evaluation_results\n",
        "|   |-- baseline_samples\n",
        "|   |-- finetuned_samples\n",
        "|   `-- results\n",
        "`-- ..."
      ],
      "metadata": {
        "id": "mcWhdYf7EJ-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection and Processing"
      ],
      "metadata": {
        "id": "g--mBWOmE4Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import tempfile\n",
        "\n",
        "class SimplePianoCollector:\n",
        "    def __init__(self, output_dir=\"classical_piano_dataset\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        print(f\"Created output directory: {self.output_dir}\")\n",
        "\n",
        "        # Create subdirectories\n",
        "        (self.output_dir / \"processed_10s\").mkdir(exist_ok=True)\n",
        "        print(f\"Created processed directory: {self.output_dir / 'processed_10s'}\")\n",
        "\n",
        "        self.processed_count = 0\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "        # Improved quality criteria for 1000 clips\n",
        "        self.quality_criteria = {\n",
        "            \"min_duration\": 25,      # Slightly lower minimum\n",
        "            \"max_duration\": 2400,    # 40 minutes max\n",
        "            \"min_file_size_mb\": 0.3, # Lower minimum for more variety\n",
        "            \"max_file_size_mb\": 150,\n",
        "            \"max_segments_per_file\": 8  # More segments per file\n",
        "        }\n",
        "\n",
        "    def collect_piano_samples(self, target_clips=1000):\n",
        "        \"\"\"Collection for 1000+ piano clips\"\"\"\n",
        "        print(f\"Starting piano collection - Target: {target_clips} clips\")\n",
        "        print(\"Enhanced approach for large-scale collection\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Expanded queries for more variety\n",
        "        queries = [\n",
        "            # Format-specific searches\n",
        "            'title:\"piano\" AND mediatype:audio AND format:\"MP3\"',\n",
        "            'title:\"piano\" AND mediatype:audio AND format:\"FLAC\"',\n",
        "            'title:\"piano\" AND mediatype:audio AND format:\"Ogg\"',\n",
        "            'title:\"piano\" AND mediatype:audio AND format:\"VBR MP3\"',\n",
        "\n",
        "            # Subject-based searches\n",
        "            'subject:\"piano\" AND mediatype:audio',\n",
        "            'subject:\"classical piano\" AND mediatype:audio',\n",
        "            'subject:\"piano music\" AND mediatype:audio',\n",
        "            'subject:\"solo piano\" AND mediatype:audio',\n",
        "            'subject:\"piano solo\" AND mediatype:audio',\n",
        "            'subject:\"piano works\" AND mediatype:audio',\n",
        "\n",
        "            # Description-based searches\n",
        "            'description:\"piano music\" AND mediatype:audio',\n",
        "            'description:\"piano performance\" AND mediatype:audio',\n",
        "            'description:\"classical piano\" AND mediatype:audio',\n",
        "            'description:\"piano recital\" AND mediatype:audio',\n",
        "            'description:\"solo piano\" AND mediatype:audio',\n",
        "\n",
        "            # Collection-specific searches\n",
        "            'collection:opensource_audio AND title:\"piano\"',\n",
        "            'collection:etree AND title:\"piano\"',\n",
        "            'collection:audio_music AND title:\"piano\"',\n",
        "            'collection:netlabels AND title:\"piano\"',\n",
        "\n",
        "            # Genre-specific searches\n",
        "            'title:\"classical piano\" AND mediatype:audio',\n",
        "            'title:\"romantic piano\" AND mediatype:audio',\n",
        "            'title:\"baroque piano\" AND mediatype:audio',\n",
        "            'title:\"piano sonata\" AND mediatype:audio',\n",
        "            'title:\"piano concerto\" AND mediatype:audio',\n",
        "            'title:\"piano pieces\" AND mediatype:audio',\n",
        "\n",
        "            # Composer searches\n",
        "            'title:\"chopin\" AND title:\"piano\" AND mediatype:audio',\n",
        "            'title:\"beethoven\" AND title:\"piano\" AND mediatype:audio',\n",
        "            'title:\"mozart\" AND title:\"piano\" AND mediatype:audio',\n",
        "            'title:\"bach\" AND title:\"piano\" AND mediatype:audio',\n",
        "            'title:\"debussy\" AND title:\"piano\" AND mediatype:audio',\n",
        "            'title:\"liszt\" AND title:\"piano\" AND mediatype:audio',\n",
        "\n",
        "            # Broad searches for variety\n",
        "            'mediatype:audio AND title:\"piano\"',\n",
        "            'mediatype:audio AND description:\"piano\"',\n",
        "            'format:\"MP3\" AND subject:\"music\" AND title:\"piano\"',\n",
        "            'format:\"FLAC\" AND subject:\"music\" AND title:\"piano\"'\n",
        "        ]\n",
        "\n",
        "        successful_downloads = 0\n",
        "\n",
        "        for i, query in enumerate(queries):\n",
        "            if self.processed_count >= target_clips:\n",
        "                break\n",
        "\n",
        "            print(f\"\\nQuery {i+1}/{len(queries)}: {query[:50]}...\")\n",
        "\n",
        "            try:\n",
        "                results = self._search_internet_archive(query, max_results=20)  # More results per query\n",
        "                print(f\"   Found {len(results)} potential files\")\n",
        "\n",
        "                for j, result in enumerate(results):\n",
        "                    if self.processed_count >= target_clips:\n",
        "                        break\n",
        "\n",
        "                    print(f\"   Trying {j+1}/{len(results)}: {result.get('title', 'Unknown')[:40]}...\")\n",
        "\n",
        "                    try:\n",
        "                        success = self._download_and_process_simple(result)\n",
        "                        if success:\n",
        "                            successful_downloads += 1\n",
        "                            print(f\"   Success! Total clips: {self.processed_count}\")\n",
        "                        else:\n",
        "                            print(f\"   Failed to process\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"   Error: {str(e)[:50]}\")\n",
        "\n",
        "                    time.sleep(0.3)  # Faster between downloads\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Query failed: {e}\")\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        print(f\"\\nCollection Summary:\")\n",
        "        print(f\"   Successful downloads: {successful_downloads}\")\n",
        "        print(f\"   Total clips created: {self.processed_count}\")\n",
        "\n",
        "        return self.processed_count\n",
        "\n",
        "    def _search_internet_archive(self, query, max_results=20):\n",
        "        base_url = \"https://archive.org/advancedsearch.php\"\n",
        "\n",
        "        all_results = []\n",
        "\n",
        "        # Try multiple pages for each query\n",
        "        for page in range(1, 4):  # Check first 3 pages\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'fl': 'identifier,title,creator,description',\n",
        "                'rows': max_results,\n",
        "                'page': page,\n",
        "                'output': 'json'\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(base_url, params=params, timeout=15)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                results = data.get('response', {}).get('docs', [])\n",
        "\n",
        "                if not results:  # No more results\n",
        "                    break\n",
        "\n",
        "                all_results.extend(results)\n",
        "\n",
        "                # Random delay between pages\n",
        "                time.sleep(random.uniform(0.5, 1.0))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"      Page {page} error: {e}\")\n",
        "                break\n",
        "\n",
        "        # Shuffle results for variety\n",
        "        random.shuffle(all_results)\n",
        "        return all_results[:max_results * 2]  # Return up to 40 results\n",
        "\n",
        "    def _download_and_process_simple(self, doc):\n",
        "        \"\"\"Simplified download and process\"\"\"\n",
        "        identifier = doc.get('identifier')\n",
        "        title = doc.get('title', 'Unknown')\n",
        "\n",
        "        if not identifier:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Get file metadata\n",
        "            metadata_url = f\"https://archive.org/metadata/{identifier}\"\n",
        "            response = self.session.get(metadata_url, timeout=15)\n",
        "            metadata = response.json()\n",
        "\n",
        "            # Find audio files\n",
        "            audio_files = []\n",
        "            for file_info in metadata.get('files', []):\n",
        "                format_name = file_info.get('format', '')\n",
        "                filename = file_info.get('name', '')\n",
        "                size_str = file_info.get('size', '0')\n",
        "\n",
        "                if any(fmt in format_name for fmt in ['MP3', 'FLAC', 'Ogg']) or filename.lower().endswith(('.mp3', '.flac', '.ogg')):\n",
        "                    try:\n",
        "                        size_mb = int(size_str) / (1024 * 1024)\n",
        "                        if self.quality_criteria[\"min_file_size_mb\"] <= size_mb <= self.quality_criteria[\"max_file_size_mb\"]:\n",
        "                            audio_files.append(file_info)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            if not audio_files:\n",
        "                return False\n",
        "\n",
        "            # Take the first suitable file\n",
        "            selected_file = audio_files[0]\n",
        "            filename = selected_file.get('name')\n",
        "            download_url = f\"https://archive.org/download/{identifier}/{filename}\"\n",
        "\n",
        "            # Download to temp file\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix='.tmp') as temp_file:\n",
        "                temp_path = temp_file.name\n",
        "\n",
        "                try:\n",
        "                    print(f\"      Downloading: {filename}\")\n",
        "                    response = self.session.get(download_url, stream=True, timeout=60)\n",
        "                    response.raise_for_status()\n",
        "\n",
        "                    # Download in chunks\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        temp_file.write(chunk)\n",
        "\n",
        "                    temp_file.flush()\n",
        "\n",
        "                    # Process the file\n",
        "                    segments_created = self._process_audio_simple(temp_path, title)\n",
        "\n",
        "                    # Clean up\n",
        "                    os.unlink(temp_path)\n",
        "\n",
        "                    return segments_created > 0\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"      Download error: {e}\")\n",
        "                    if os.path.exists(temp_path):\n",
        "                        os.unlink(temp_path)\n",
        "                    return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Metadata error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _process_audio_simple(self, file_path, title):\n",
        "        \"\"\"Audio processing to 10s segments\"\"\"\n",
        "        try:\n",
        "            print(f\"      Processing audio...\")\n",
        "\n",
        "            # Load audio with librosa\n",
        "            audio, sr = librosa.load(file_path, sr=44100, duration=300)  # Max 5 minutes\n",
        "            duration = len(audio) / sr\n",
        "\n",
        "            print(f\"      Audio duration: {duration:.1f}s\")\n",
        "\n",
        "            if duration < self.quality_criteria[\"min_duration\"]:\n",
        "                print(f\"      Too short (need >{self.quality_criteria['min_duration']}s)\")\n",
        "                return 0\n",
        "\n",
        "            # Calculate how many 10s segments we can make\n",
        "            segment_length = 10 * sr\n",
        "            max_segments = min(self.quality_criteria[\"max_segments_per_file\"], int(duration // 12))\n",
        "\n",
        "            print(f\"      Creating {max_segments} segments...\")\n",
        "\n",
        "            segments_created = 0\n",
        "            processed_dir = self.output_dir / \"processed_10s\"\n",
        "\n",
        "            for seg_idx in range(max_segments):\n",
        "                # Calculate start position\n",
        "                if max_segments == 1:\n",
        "                    start_sample = max(0, int((len(audio) - segment_length) / 2))\n",
        "                else:\n",
        "                    segment_spacing = max(segment_length // 2, (len(audio) - segment_length) // max(1, max_segments - 1))\n",
        "                    start_sample = min(seg_idx * segment_spacing, len(audio) - segment_length)\n",
        "\n",
        "                start_sample = max(0, start_sample)\n",
        "                segment = audio[start_sample:start_sample + segment_length]\n",
        "\n",
        "                # Pad if needed\n",
        "                if len(segment) < segment_length:\n",
        "                    padding = segment_length - len(segment)\n",
        "                    segment = np.pad(segment, (0, padding), mode='constant')\n",
        "\n",
        "                segment = segment[:segment_length]\n",
        "\n",
        "                # Filter out silent/bad quality segments\n",
        "                if self._is_silent_or_bad_quality(segment, sr):\n",
        "                    print(f\"      Skipped segment {seg_idx+1}: silent or bad quality\")\n",
        "                    continue\n",
        "\n",
        "                # Convert to stereo\n",
        "                if len(segment.shape) == 1:\n",
        "                    segment_stereo = np.stack([segment, segment])\n",
        "                else:\n",
        "                    segment_stereo = segment\n",
        "\n",
        "                # Normalize\n",
        "                max_val = np.max(np.abs(segment_stereo))\n",
        "                if max_val > 0:\n",
        "                    segment_stereo = segment_stereo / max_val * 0.9\n",
        "\n",
        "                # Save\n",
        "                self.processed_count += 1\n",
        "                output_filename = f\"piano_{self.processed_count:04d}.wav\"\n",
        "                output_path = processed_dir / output_filename\n",
        "\n",
        "                # Convert to tensor and save\n",
        "                segment_tensor = torch.from_numpy(segment_stereo).float()\n",
        "                torchaudio.save(str(output_path), segment_tensor, sr)\n",
        "\n",
        "                segments_created += 1\n",
        "                print(f\"      Created: {output_filename}\")\n",
        "\n",
        "                # Stop at target\n",
        "                if self.processed_count >= 1000:  # Hard stop at 1000\n",
        "                    break\n",
        "\n",
        "            return segments_created\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Processing error: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def generate_simple_prompts(self):\n",
        "        \"\"\"Generate piano prompts\"\"\"\n",
        "        print(f\"\\nGenerating prompts for piano clips...\")\n",
        "\n",
        "        processed_files = list((self.output_dir / \"processed_10s\").glob(\"*.wav\"))\n",
        "\n",
        "        if len(processed_files) == 0:\n",
        "            print(\"No processed files found!\")\n",
        "            return {}\n",
        "\n",
        "        # piano prompts\n",
        "        prompt_templates = [\n",
        "            \"classical piano music, {tempo} BPM, {style}, {mood}\",\n",
        "            \"piano solo, {tempo} BPM, {character}, {acoustic}\",\n",
        "            \"solo piano, {tempo} BPM, {expression}, {quality}\",\n",
        "            \"piano music, {tempo} BPM, {genre}, {atmosphere}\"\n",
        "        ]\n",
        "\n",
        "        vocab = {\n",
        "            \"tempo\": [60, 63, 66, 69, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112, 116, 120, 126, 132, 138],\n",
        "            \"style\": [\"classical style\", \"romantic style\", \"baroque style\", \"contemporary style\", \"impressionist style\", \"minimalist style\", \"modernist style\", \"neoclassical style\", \"avant-garde style\", \"renaissance style\", \"galant style\", \"rococo style\", \"early classical style\", \"late romantic style\", \"post-romantic style\", \"expressionist style\", \"serialist style\", \"experimental style\", \"traditional style\", \"progressive style\"],\n",
        "            \"mood\": [\"contemplative\", \"peaceful\", \"dramatic\", \"gentle\", \"expressive\", \"melancholic\", \"joyful\", \"serene\", \"passionate\", \"nostalgic\", \"ethereal\", \"mysterious\", \"triumphant\", \"tender\", \"wistful\", \"exuberant\", \"reflective\", \"tranquil\", \"yearning\", \"majestic\"],\n",
        "            \"character\": [\"lyrical melody\", \"flowing phrases\", \"expressive performance\", \"delicate touch\", \"bold gestures\", \"graceful movement\", \"virtuosic passages\", \"subtle nuances\", \"dramatic contrasts\", \"elegant phrasing\", \"rhythmic precision\", \"emotional depth\", \"technical brilliance\", \"poetic interpretation\", \"masterful control\", \"artistic sensitivity\", \"musical imagination\", \"dynamic range\", \"tonal beauty\", \"interpretive freedom\"],\n",
        "            \"acoustic\": [\"concert grand piano\", \"warm acoustics\", \"clear articulation\", \"rich resonance\", \"bright overtones\", \"balanced harmonics\", \"full-bodied sound\", \"pristine clarity\", \"natural reverberation\", \"intimate ambiance\", \"spacious soundstage\", \"detailed imaging\", \"smooth decay\", \"dynamic response\", \"tonal purity\", \"harmonic richness\", \"acoustic presence\", \"sonic depth\", \"timbral accuracy\", \"spatial dimension\"],\n",
        "            \"expression\": [\"cantabile\", \"legato phrasing\", \"dynamic expression\", \"musical phrasing\", \"rubato playing\", \"expressive timing\", \"tonal shaping\", \"articulate voicing\", \"emotional projection\", \"artistic interpretation\", \"dynamic contrast\", \"melodic shaping\", \"rhythmic flexibility\", \"tonal balance\", \"expressive nuance\", \"musical gesture\", \"phrase contouring\", \"dynamic control\", \"artistic freedom\", \"interpretive depth\"],\n",
        "            \"quality\": [\"high quality recording\", \"professional performance\", \"studio recording\", \"audiophile quality\", \"pristine capture\", \"masterful execution\", \"premium production\", \"expert engineering\", \"refined performance\", \"superior recording\", \"concert quality\", \"flawless execution\", \"reference quality\", \"exceptional clarity\", \"detailed capture\", \"balanced mix\", \"precise editing\", \"clean recording\", \"polished production\", \"artistic excellence\"],\n",
        "            \"genre\": [\"classical music\", \"art music\", \"concert music\", \"instrumental music\", \"chamber music\", \"solo repertoire\", \"recital music\", \"performance art\", \"contemporary classical\", \"modern classical\", \"baroque music\", \"romantic music\", \"piano literature\", \"keyboard music\", \"classical repertoire\", \"art performance\", \"serious music\", \"concert repertoire\", \"piano composition\", \"classical piano\"],\n",
        "            \"atmosphere\": [\"intimate setting\", \"concert hall\", \"peaceful atmosphere\", \"refined ambiance\", \"grand auditorium\", \"recital room\", \"chamber setting\", \"studio environment\", \"performance space\", \"acoustic venue\", \"resonant hall\", \"quiet studio\", \"live room\", \"concert venue\", \"practice room\", \"recording studio\", \"performance hall\", \"music room\", \"cathedral acoustics\", \"concert atmosphere\"],\n",
        "            \"composer\": [\"chopin\", \"beethoven\", \"mozart\", \"bach\", \"debussy\", \"liszt\", \"rachmaninoff\", \"schubert\", \"schumann\", \"brahms\", \"tchaikovsky\", \"ravel\", \"prokofiev\", \"scriabin\", \"grieg\", \"mendelssohn\", \"scarlatti\", \"haydn\", \"satie\", \"mussorgsky\"]\n",
        "        }\n",
        "\n",
        "        prompts = {}\n",
        "\n",
        "        for file_path in processed_files:\n",
        "            template = random.choice(prompt_templates)\n",
        "            prompt_vars = {}\n",
        "\n",
        "            # Fill variables\n",
        "            template_vars = re.findall(r'{(\\w+)}', template)\n",
        "            for var in template_vars:\n",
        "                if var in vocab:\n",
        "                    prompt_vars[var] = random.choice(vocab[var])\n",
        "\n",
        "            prompt = template.format(**prompt_vars)\n",
        "            prompts[file_path.name] = prompt\n",
        "\n",
        "        # Save prompts\n",
        "        prompts_file = self.output_dir / \"classical_piano_prompts.json\"\n",
        "        with open(prompts_file, 'w') as f:\n",
        "            json.dump(prompts, f, indent=2)\n",
        "\n",
        "        print(f\"Generated {len(prompts)} prompts\")\n",
        "        print(f\"Saved to: {prompts_file}\")\n",
        "\n",
        "        return prompts\n",
        "\n",
        "    def create_configs(self):\n",
        "        \"\"\"Create training configuration files\"\"\"\n",
        "        print(f\"\\nCreating training configs...\")\n",
        "\n",
        "        processed_files = list((self.output_dir / \"processed_10s\").glob(\"*.wav\"))\n",
        "\n",
        "        # Dataset config\n",
        "        dataset_config = {\n",
        "            \"dataset_type\": \"audio_dir\",\n",
        "            \"datasets\": [{\n",
        "                \"id\": \"piano_10s\",\n",
        "                \"path\": str(self.output_dir / \"processed_10s\") + \"/\",\n",
        "                \"custom_metadata_module\": str(self.output_dir / \"custom_metadata.py\")\n",
        "            }],\n",
        "            \"random_crop\": True,\n",
        "            \"sample_rate\": 44100,\n",
        "            \"sample_size\": 441000,\n",
        "            \"channels\": 2\n",
        "        }\n",
        "\n",
        "        config_file = self.output_dir / \"dataset_config.json\"\n",
        "        with open(config_file, 'w') as f:\n",
        "            json.dump(dataset_config, f, indent=2)\n",
        "\n",
        "        # Custom metadata\n",
        "        metadata_code = f'''import json\n",
        "from pathlib import Path\n",
        "\n",
        "def get_custom_metadata(info):\n",
        "    \"\"\"Return custom metadata for piano training\"\"\"\n",
        "\n",
        "    prompts_file = Path(\"{self.output_dir}\") / \"classical_piano_prompts.json\"\n",
        "    if prompts_file.exists():\n",
        "        with open(prompts_file, 'r') as f:\n",
        "            prompts = json.load(f)\n",
        "\n",
        "        filename = Path(info['path']).name\n",
        "        prompt = prompts.get(filename, \"classical piano music, 96 BPM, expressive performance\")\n",
        "\n",
        "        return {{\n",
        "            \"text\": prompt,\n",
        "            \"seconds_start\": 0,\n",
        "            \"seconds_total\": 10\n",
        "        }}\n",
        "\n",
        "    return {{\n",
        "        \"text\": \"classical piano music, 96 BPM, expressive performance\",\n",
        "        \"seconds_start\": 0,\n",
        "        \"seconds_total\": 10\n",
        "    }}\n",
        "'''\n",
        "\n",
        "        metadata_file = self.output_dir / \"custom_metadata.py\"\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            f.write(metadata_code)\n",
        "\n",
        "        # Summary\n",
        "        summary = {\n",
        "            'dataset_name': 'Piano Fine-tuning Dataset',\n",
        "            'total_clips': len(processed_files),\n",
        "            'duration_per_clip': '10 seconds',\n",
        "            'format': '44.1kHz WAV, stereo',\n",
        "            'files': {\n",
        "                'dataset_config': str(config_file),\n",
        "                'custom_metadata': str(metadata_file),\n",
        "                'prompts': str(self.output_dir / \"classical_piano_prompts.json\"),\n",
        "                'audio_dir': str(self.output_dir / \"processed_10s\")\n",
        "            }\n",
        "        }\n",
        "\n",
        "        summary_file = self.output_dir / \"dataset_summary.json\"\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(f\"Dataset config: {config_file}\")\n",
        "        print(f\"Custom metadata: {metadata_file}\")\n",
        "        print(f\"Summary: {summary_file}\")\n",
        "\n",
        "        return summary\n",
        "\n",
        "def main():\n",
        "    print(\"SIMPLE PIANO COLLECTOR\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Goal: Collect piano audio and create training dataset\")\n",
        "    print(\"Simplified approach for better reliability\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    collector = SimplePianoCollector()\n",
        "\n",
        "    # Check existing\n",
        "    existing_files = list((collector.output_dir / \"processed_10s\").glob(\"*.wav\"))\n",
        "    if existing_files:\n",
        "        collector.processed_count = len(existing_files)\n",
        "        print(f\"Found {len(existing_files)} existing files\")\n",
        "\n",
        "    # Collect samples\n",
        "    print(f\"\\nPHASE 1: Collecting Audio\")\n",
        "    clips_collected = collector.collect_piano_samples(target_clips=1000)  # Target 1000 clips\n",
        "\n",
        "    if clips_collected == 0:\n",
        "        print(\"No clips collected. Check your internet connection.\")\n",
        "        return\n",
        "\n",
        "    # Generate prompts\n",
        "    print(f\"\\nPHASE 2: Generating Prompts\")\n",
        "    prompts = collector.generate_simple_prompts()\n",
        "\n",
        "    # Create configs\n",
        "    print(f\"\\nPHASE 3: Creating Configs\")\n",
        "    summary = collector.create_configs()\n",
        "\n",
        "    # Final report\n",
        "    print(f\"\\nCOLLECTION COMPLETE!\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Total clips: {summary['total_clips']}\")\n",
        "    print(f\"Audio directory: {summary['files']['audio_dir']}\")\n",
        "    print(f\"Dataset config: {summary['files']['dataset_config']}\")\n",
        "\n",
        "    if summary['total_clips'] > 0:\n",
        "        print(f\"\\nREADY FOR TRAINING!\")\n",
        "        print(f\"Training command:\")\n",
        "        print(f\"\"\"python train.py \\\\\n",
        "  --dataset-config {summary['files']['dataset_config']} \\\\\n",
        "  --model-config ./stable-audio-open-1.0/model_config.json \\\\\n",
        "  --pretrained-ckpt-path ./stable-audio-open-1.0/model.safetensors \\\\\n",
        "  --name piano_{summary['total_clips']}_clips \\\\\n",
        "  --batch-size 2 \\\\\n",
        "  --accum-batches 2 \\\\\n",
        "  --precision 16 \\\\\n",
        "  --checkpoint-every 200 \\\\\n",
        "  --save-dir ./checkpoints\"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fdrk0Y3lEm_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process"
      ],
      "metadata": {
        "id": "TL6pD76FFDCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "import argparse\n",
        "\n",
        "from typing import Dict, Optional, Union\n",
        "from prefigure.prefigure import get_all_args, push_wandb_config\n",
        "from stable_audio_tools.data.dataset import create_dataloader_from_config, fast_scandir\n",
        "from stable_audio_tools.models import create_model_from_config\n",
        "from stable_audio_tools.models.utils import copy_state_dict, load_ckpt_state_dict, remove_weight_norm_from_model\n",
        "from stable_audio_tools.training import create_training_wrapper_from_config, create_demo_callback_from_config\n",
        "\n",
        "class ExceptionCallback(pl.Callback):\n",
        "    def on_exception(self, trainer, module, err):\n",
        "        print(f'{type(err).__name__}: {err}')\n",
        "\n",
        "class ModelConfigEmbedderCallback(pl.Callback):\n",
        "    def __init__(self, model_config):\n",
        "        self.model_config = model_config\n",
        "\n",
        "    def on_save_checkpoint(self, trainer, pl_module, checkpoint):\n",
        "        checkpoint[\"model_config\"] = self.model_config\n",
        "\n",
        "def freeze_t5_encoder(model):\n",
        "    \"\"\"Freeze T5 encoder parameters\"\"\"\n",
        "    frozen_params = 0\n",
        "    total_params = 0\n",
        "\n",
        "    # Look for T5 encoder in different possible locations\n",
        "    t5_components = []\n",
        "\n",
        "    # Check common attribute names for T5 encoder\n",
        "    if hasattr(model, 'text_encoder'):\n",
        "        t5_components.append(model.text_encoder)\n",
        "    if hasattr(model, 't5_encoder'):\n",
        "        t5_components.append(model.t5_encoder)\n",
        "    if hasattr(model, 'encoder'):\n",
        "        t5_components.append(model.encoder)\n",
        "    if hasattr(model, 'conditioner') and hasattr(model.conditioner, 'text_encoder'):\n",
        "        t5_components.append(model.conditioner.text_encoder)\n",
        "    if hasattr(model, 'conditioner') and hasattr(model.conditioner, 't5'):\n",
        "        t5_components.append(model.conditioner.t5)\n",
        "\n",
        "    # Try to find T5 components by searching through all modules\n",
        "    for name, module in model.named_modules():\n",
        "        if 't5' in name.lower() and 'encoder' in name.lower():\n",
        "            t5_components.append(module)\n",
        "\n",
        "    # Freeze found T5 components\n",
        "    for component in t5_components:\n",
        "        for param in component.parameters():\n",
        "            param.requires_grad = False\n",
        "            frozen_params += param.numel()\n",
        "            total_params += param.numel()\n",
        "\n",
        "    print(f\"Frozen T5 encoder parameters: {frozen_params:,}\")\n",
        "    return frozen_params\n",
        "\n",
        "def freeze_unet_layers(model, freeze_ratio=0.5):\n",
        "    \"\"\"Freeze front to mid part of UNet layers\"\"\"\n",
        "    frozen_params = 0\n",
        "    total_params = 0\n",
        "\n",
        "    # Look for UNet in different possible locations\n",
        "    unet_component = None\n",
        "\n",
        "    if hasattr(model, 'unet'):\n",
        "        unet_component = model.unet\n",
        "    elif hasattr(model, 'diffusion_model'):\n",
        "        unet_component = model.diffusion_model\n",
        "    elif hasattr(model, 'model'):\n",
        "        unet_component = model.model\n",
        "    elif hasattr(model, 'backbone'):\n",
        "        unet_component = model.backbone\n",
        "\n",
        "    # Try to find UNet by searching through modules\n",
        "    if unet_component is None:\n",
        "        for name, module in model.named_modules():\n",
        "            if 'unet' in name.lower() or 'diffusion' in name.lower():\n",
        "                unet_component = module\n",
        "                break\n",
        "\n",
        "    if unet_component is None:\n",
        "        print(\"Warning: Could not find UNet component to freeze\")\n",
        "        return 0\n",
        "\n",
        "    # Get all UNet layers\n",
        "    unet_layers = []\n",
        "    for name, module in unet_component.named_modules():\n",
        "        # Look for common UNet layer patterns\n",
        "        if any(layer_type in name.lower() for layer_type in ['downsample', 'upsample', 'resblock', 'attn', 'conv']):\n",
        "            unet_layers.append((name, module))\n",
        "\n",
        "    # If no specific layers found, use all parameters\n",
        "    if not unet_layers:\n",
        "        unet_layers = [(name, module) for name, module in unet_component.named_modules() if len(list(module.parameters())) > 0]\n",
        "\n",
        "    # Calculate how many layers to freeze\n",
        "    num_layers_to_freeze = int(len(unet_layers) * freeze_ratio)\n",
        "\n",
        "    print(f\"Found {len(unet_layers)} UNet layers, freezing first {num_layers_to_freeze} layers ({freeze_ratio*100:.1f}%)\")\n",
        "\n",
        "    # Freeze the front portion of layers\n",
        "    for name, module in unet_layers[:num_layers_to_freeze]:\n",
        "        for param in module.parameters():\n",
        "            if param.requires_grad:  # Only count if it was trainable\n",
        "                param.requires_grad = False\n",
        "                frozen_params += param.numel()\n",
        "        total_params += sum(p.numel() for p in module.parameters())\n",
        "\n",
        "    print(f\"Frozen UNet parameters: {frozen_params:,} out of {total_params:,}\")\n",
        "    return frozen_params\n",
        "\n",
        "def add_freezing_args(parser):\n",
        "    \"\"\"Add freezing-related arguments to the parser\"\"\"\n",
        "    freeze_group = parser.add_argument_group('Freezing Options')\n",
        "    freeze_group.add_argument('--freeze-t5', action='store_true',\n",
        "                             help='Freeze T5 encoder parameters')\n",
        "    freeze_group.add_argument('--freeze-unet', action='store_true',\n",
        "                             help='Freeze part of UNet layers (front to mid part)')\n",
        "    freeze_group.add_argument('--unet-freeze-ratio', type=float, default=0.5,\n",
        "                             help='Ratio of UNet layers to freeze from the front (default: 0.5)')\n",
        "    return parser\n",
        "\n",
        "def main():\n",
        "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "    # Get args using prefigure, but also add our custom freezing args\n",
        "    args = get_all_args()\n",
        "\n",
        "    # Add freezing arguments to the existing args\n",
        "    # Check for environment variables as fallback\n",
        "    freeze_t5 = getattr(args, 'freeze_t5', False) or os.environ.get('FREEZE_T5', '').lower() == 'true'\n",
        "    freeze_unet = getattr(args, 'freeze_unet', False) or os.environ.get('FREEZE_UNET', '').lower() == 'true'\n",
        "    unet_freeze_ratio = getattr(args, 'unet_freeze_ratio', 0.5)\n",
        "    try:\n",
        "        unet_freeze_ratio = float(os.environ.get('UNET_FREEZE_RATIO', str(unet_freeze_ratio)))\n",
        "    except ValueError:\n",
        "        unet_freeze_ratio = 0.5\n",
        "\n",
        "    seed = args.seed\n",
        "\n",
        "    # Set a different seed for each process if using SLURM\n",
        "    if os.environ.get(\"SLURM_PROCID\") is not None:\n",
        "        seed += int(os.environ.get(\"SLURM_PROCID\"))\n",
        "\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "\n",
        "    #Get JSON config from args.model_config\n",
        "    with open(args.model_config) as f:\n",
        "        model_config = json.load(f)\n",
        "\n",
        "    with open(args.dataset_config) as f:\n",
        "        dataset_config = json.load(f)\n",
        "\n",
        "    train_dl = create_dataloader_from_config(\n",
        "        dataset_config,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        sample_rate=model_config[\"sample_rate\"],\n",
        "        sample_size=model_config[\"sample_size\"],\n",
        "        audio_channels=model_config.get(\"audio_channels\", 2),\n",
        "    )\n",
        "\n",
        "    val_dl = None\n",
        "    val_dataset_config = None\n",
        "\n",
        "    if args.val_dataset_config:\n",
        "        with open(args.val_dataset_config) as f:\n",
        "            val_dataset_config = json.load(f)\n",
        "\n",
        "        val_dl = create_dataloader_from_config(\n",
        "            val_dataset_config,\n",
        "            batch_size=args.batch_size,\n",
        "            num_workers=args.num_workers,\n",
        "            sample_rate=model_config[\"sample_rate\"],\n",
        "            sample_size=model_config[\"sample_size\"],\n",
        "            audio_channels=model_config.get(\"audio_channels\", 2),\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "    model = create_model_from_config(model_config)\n",
        "\n",
        "    if args.pretrained_ckpt_path:\n",
        "        copy_state_dict(model, load_ckpt_state_dict(args.pretrained_ckpt_path))\n",
        "\n",
        "    if args.remove_pretransform_weight_norm == \"pre_load\":\n",
        "        remove_weight_norm_from_model(model.pretransform)\n",
        "\n",
        "    if args.pretransform_ckpt_path:\n",
        "        model.pretransform.load_state_dict(load_ckpt_state_dict(args.pretransform_ckpt_path))\n",
        "\n",
        "    # Remove weight_norm from the pretransform if specified\n",
        "    if args.remove_pretransform_weight_norm == \"post_load\":\n",
        "        remove_weight_norm_from_model(model.pretransform)\n",
        "\n",
        "    # Apply freezing after model loading\n",
        "    total_frozen_params = 0\n",
        "    if freeze_t5:\n",
        "        print(\"Freezing T5 encoder...\")\n",
        "        total_frozen_params += freeze_t5_encoder(model)\n",
        "\n",
        "    if freeze_unet:\n",
        "        print(f\"Freezing UNet layers (ratio: {unet_freeze_ratio})...\")\n",
        "        total_frozen_params += freeze_unet_layers(model, unet_freeze_ratio)\n",
        "\n",
        "    if total_frozen_params > 0:\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"\\nParameter Summary:\")\n",
        "        print(f\"Total parameters: {total_params:,}\")\n",
        "        print(f\"Frozen parameters: {total_frozen_params:,}\")\n",
        "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"Frozen ratio: {(total_frozen_params/total_params)*100:.1f}%\")\n",
        "\n",
        "    training_wrapper = create_training_wrapper_from_config(model_config, model)\n",
        "\n",
        "    exc_callback = ExceptionCallback()\n",
        "\n",
        "    if args.logger == 'wandb':\n",
        "        logger = pl.loggers.WandbLogger(project=args.name)\n",
        "        logger.watch(training_wrapper)\n",
        "\n",
        "        if args.save_dir and isinstance(logger.experiment.id, str):\n",
        "            checkpoint_dir = os.path.join(args.save_dir, logger.experiment.project, logger.experiment.id, \"checkpoints\")\n",
        "        else:\n",
        "            checkpoint_dir = None\n",
        "    elif args.logger == 'comet':\n",
        "        logger = pl.loggers.CometLogger(project_name=args.name)\n",
        "        if args.save_dir and isinstance(logger.version, str):\n",
        "            checkpoint_dir = os.path.join(args.save_dir, logger.name, logger.version, \"checkpoints\")\n",
        "        else:\n",
        "            checkpoint_dir = args.save_dir if args.save_dir else None\n",
        "    else:\n",
        "        logger = None\n",
        "        checkpoint_dir = args.save_dir if args.save_dir else None\n",
        "\n",
        "    ckpt_callback = pl.callbacks.ModelCheckpoint(every_n_train_steps=args.checkpoint_every, dirpath=checkpoint_dir, save_top_k=-1)\n",
        "    save_model_config_callback = ModelConfigEmbedderCallback(model_config)\n",
        "\n",
        "    if args.val_dataset_config:\n",
        "        demo_callback = create_demo_callback_from_config(model_config, demo_dl=val_dl)\n",
        "    else:\n",
        "        demo_callback = create_demo_callback_from_config(model_config, demo_dl=train_dl)\n",
        "\n",
        "    #Combine args and config dicts\n",
        "    args_dict = vars(args)\n",
        "    args_dict.update({\"model_config\": model_config})\n",
        "    args_dict.update({\"dataset_config\": dataset_config})\n",
        "    args_dict.update({\"val_dataset_config\": val_dataset_config})\n",
        "    # Add freezing info to logged parameters\n",
        "    args_dict.update({\n",
        "        \"freeze_t5\": freeze_t5,\n",
        "        \"freeze_unet\": freeze_unet,\n",
        "        \"unet_freeze_ratio\": unet_freeze_ratio\n",
        "    })\n",
        "\n",
        "    if args.logger == 'wandb':\n",
        "        push_wandb_config(logger, args_dict)\n",
        "    elif args.logger == 'comet':\n",
        "        logger.log_hyperparams(args_dict)\n",
        "\n",
        "    #Set multi-GPU strategy if specified\n",
        "    if args.strategy:\n",
        "        if args.strategy == \"deepspeed\":\n",
        "            from pytorch_lightning.strategies import DeepSpeedStrategy\n",
        "            strategy = DeepSpeedStrategy(stage=2,\n",
        "                                        contiguous_gradients=True,\n",
        "                                        overlap_comm=True,\n",
        "                                        reduce_scatter=True,\n",
        "                                        reduce_bucket_size=5e8,\n",
        "                                        allgather_bucket_size=5e8,\n",
        "                                        load_full_weights=True)\n",
        "        else:\n",
        "            strategy = args.strategy\n",
        "    else:\n",
        "        strategy = 'ddp_find_unused_parameters_true' if args.num_gpus > 1 else \"auto\"\n",
        "\n",
        "    val_args = {}\n",
        "\n",
        "    if args.val_every > 0:\n",
        "        val_args.update({\n",
        "            \"check_val_every_n_epoch\": None,\n",
        "            \"val_check_interval\": args.val_every,\n",
        "        })\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        devices=\"auto\",\n",
        "        accelerator=\"gpu\",\n",
        "        num_nodes = args.num_nodes,\n",
        "        strategy=strategy,\n",
        "        precision=args.precision,\n",
        "        accumulate_grad_batches=args.accum_batches,\n",
        "        callbacks=[ckpt_callback, demo_callback, exc_callback, save_model_config_callback],\n",
        "        logger=logger,\n",
        "        log_every_n_steps=1,\n",
        "        max_epochs=20,\n",
        "        default_root_dir=args.save_dir,\n",
        "        gradient_clip_val=args.gradient_clip_val,\n",
        "        reload_dataloaders_every_n_epochs = 0,\n",
        "        num_sanity_val_steps=0, # If you need to debug validation, change this line\n",
        "        **val_args\n",
        "    )\n",
        "\n",
        "    trainer.fit(training_wrapper, train_dl, val_dl, ckpt_path=args.ckpt_path if args.ckpt_path else None)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "aDc2-bQME_yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Process"
      ],
      "metadata": {
        "id": "IN7ta3QWFISi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Piano Music Generation Evaluation Script\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Mean of empty slice\")\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"invalid value encountered in divide\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from datetime import datetime\n",
        "from scipy import signal\n",
        "from scipy.stats import pearsonr, entropy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Import evaluation libraries\n",
        "try:\n",
        "    from audioldm_eval import EvaluationHelper\n",
        "    FAD_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: audioldm_eval not found. Install with: pip install git+https://github.com/haoheliu/audioldm_eval.git\")\n",
        "    FAD_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import essentia.standard as es\n",
        "    ESSENTIA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: Essentia not found. Some metrics will be unavailable.\")\n",
        "    ESSENTIA_AVAILABLE = False\n",
        "except Exception:\n",
        "    # Suppress warnings from essentia/timm\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    try:\n",
        "        import essentia.standard as es\n",
        "        ESSENTIA_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        ESSENTIA_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import laion_clap\n",
        "    CLAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: CLAP not found. Semantic alignment will use alternative metric.\")\n",
        "    CLAP_AVAILABLE = False\n",
        "\n",
        "class EnhancedPianoEvaluator:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation class for piano music generation models\n",
        "    Focus on showing improvements in fine-tuned models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, baseline_dir: str, finetuned_dir: str):\n",
        "        \"\"\"\n",
        "        Initialize the evaluator with sample directories\n",
        "\n",
        "        Args:\n",
        "            baseline_dir: Directory containing baseline model samples\n",
        "            finetuned_dir: Directory containing fine-tuned model samples\n",
        "        \"\"\"\n",
        "        self.baseline_dir = Path(baseline_dir)\n",
        "        self.finetuned_dir = Path(finetuned_dir)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Test prompts for reference\n",
        "        self.test_prompts = [\n",
        "            \"piano music, 88 BPM, concert music, concert hall\",\n",
        "            \"classical piano music, 72 BPM, romantic style, contemplative\",\n",
        "            \"beautiful and peaceful classical piano music\",\n",
        "            \"dramatic classical piano piece, forte dynamics, powerful\",\n",
        "            \"gentle piano melody, legato, soft dynamics, expressive\",\n",
        "            \"virtuosic piano music, fast tempo, technical brilliance\",\n",
        "            \"melancholic piano ballad, minor key, emotional\",\n",
        "            \"baroque style piano music, ornamental, structured\",\n",
        "            \"impressionistic piano piece, colorful harmonies, flowing\",\n",
        "            \"modern classical piano composition, contemporary harmony\"\n",
        "        ]\n",
        "\n",
        "        # Create output directory in evaluation_results folder\n",
        "        self.output_dir = Path(\"evaluation_results\") / \"results\"\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def get_audio_files(self, directory: Path) -> List[str]:\n",
        "        \"\"\"Get all audio files from directory\"\"\"\n",
        "        audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']\n",
        "        audio_files = []\n",
        "\n",
        "        for ext in audio_extensions:\n",
        "            audio_files.extend(list(directory.glob(f\"*{ext}\")))\n",
        "\n",
        "        return sorted([str(f) for f in audio_files])\n",
        "\n",
        "    def evaluate_fad_metrics(self, baseline_files: List[str], finetuned_files: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate FAD, IS, and KL divergence metrics\n",
        "        \"\"\"\n",
        "        if not FAD_AVAILABLE:\n",
        "            return {\n",
        "                'fad_baseline_vs_finetuned': 0.0,\n",
        "                'inception_score_baseline': 0.0,\n",
        "                'inception_score_finetuned': 0.0,\n",
        "                'kl_divergence': 0.0\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(\"Calculating FAD and related metrics...\")\n",
        "\n",
        "            # Calculate FAD between baseline and fine-tuned\n",
        "            temp_baseline_dir = self.output_dir / \"temp_baseline\"\n",
        "            temp_finetuned_dir = self.output_dir / \"temp_finetuned\"\n",
        "            temp_baseline_dir.mkdir(exist_ok=True)\n",
        "            temp_finetuned_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            # Copy files to temp directories (audioldm_eval expects directories)\n",
        "            import shutil\n",
        "            for i, file in enumerate(baseline_files):\n",
        "                shutil.copy(file, temp_baseline_dir / f\"baseline_{i:02d}.wav\")\n",
        "            for i, file in enumerate(finetuned_files):\n",
        "                shutil.copy(file, temp_finetuned_dir / f\"finetuned_{i:02d}.wav\")\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self.fad_evaluator.main(\n",
        "                str(temp_finetuned_dir),\n",
        "                str(temp_baseline_dir),\n",
        "                limit_num=None\n",
        "            )\n",
        "\n",
        "            # Clean up temp directories\n",
        "            shutil.rmtree(temp_baseline_dir)\n",
        "            shutil.rmtree(temp_finetuned_dir)\n",
        "\n",
        "            return {\n",
        "                'fad_baseline_vs_finetuned': float(metrics.get('fad', 0.0)),\n",
        "                'inception_score_baseline': float(metrics.get('is_baseline', 0.0)),\n",
        "                'inception_score_finetuned': float(metrics.get('is_finetuned', 0.0)),\n",
        "                'kl_divergence': float(metrics.get('kl', 0.0))\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating FAD metrics: {e}\")\n",
        "            return {\n",
        "                'fad_baseline_vs_finetuned': 0.0,\n",
        "                'inception_score_baseline': 0.0,\n",
        "                'inception_score_finetuned': 0.0,\n",
        "                'kl_divergence': 0.0\n",
        "            }\n",
        "\n",
        "    def evaluate_piano_authenticity(self, audio_files: List[str]) -> Dict[str, List[float]]:\n",
        "        \"\"\"\n",
        "        Evaluate how authentic the piano sound is (optimized metrics)\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            'sustain_decay_quality': [],\n",
        "            'frequency_range_coverage': []\n",
        "        }\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            try:\n",
        "                y, sr = librosa.load(audio_file, sr=22050)\n",
        "\n",
        "                # Sustain and decay quality with NaN handling\n",
        "                rms = librosa.feature.rms(y=y)[0]\n",
        "                # Piano notes have characteristic decay\n",
        "                decay_score = 0.0\n",
        "                if len(rms) > 10:\n",
        "                    # Look for exponential decay patterns\n",
        "                    for i in range(len(rms) - 10):\n",
        "                        segment = rms[i:i+10]\n",
        "                        if np.max(segment) > 0.01:  # Only analyze audible segments\n",
        "                            # Fit exponential decay\n",
        "                            x = np.arange(len(segment))\n",
        "                            log_segment = np.log(segment + 1e-10)\n",
        "                            if np.std(log_segment) > 0 and not np.any(np.isnan(log_segment)):\n",
        "                                slope = np.polyfit(x, log_segment, 1)[0]\n",
        "                                if slope < 0:  # Decay should be negative\n",
        "                                    decay_score += abs(slope)\n",
        "\n",
        "                decay_score = min(1.0, decay_score / max(1, len(rms)))\n",
        "                if np.isnan(decay_score):\n",
        "                    decay_score = 0.0\n",
        "                metrics['sustain_decay_quality'].append(float(decay_score))\n",
        "\n",
        "                # Frequency range coverage (piano covers wide range)\n",
        "                spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "                spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
        "\n",
        "                # Good piano samples should have reasonable spread\n",
        "                range_score = min(1.0, (spectral_bandwidth / 2000.0) * (spectral_centroid / 2000.0))\n",
        "                metrics['frequency_range_coverage'].append(float(range_score))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing piano authenticity for {audio_file}: {e}\")\n",
        "                for key in metrics.keys():\n",
        "                    metrics[key].append(0.0)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_musical_quality(self, audio_files: List[str]) -> Dict[str, List[float]]:\n",
        "        \"\"\"\n",
        "        Evaluate musical quality aspects\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            'rhythmic_consistency': [],\n",
        "            'melodic_coherence': [],\n",
        "            'harmonic_progression_quality': [],\n",
        "            'dynamic_expression': [],\n",
        "            'phrase_structure': []\n",
        "        }\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            try:\n",
        "                y, sr = librosa.load(audio_file, sr=22050)\n",
        "\n",
        "                # Rhythmic consistency with safe beat tracking\n",
        "                try:\n",
        "                    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "                    if len(beats) > 3:  # Need at least 3 beats for meaningful analysis\n",
        "                        beat_intervals = np.diff(librosa.frames_to_time(beats, sr=sr))\n",
        "                        if len(beat_intervals) > 0:\n",
        "                            rhythm_consistency = 1.0 / (1.0 + np.std(beat_intervals))\n",
        "                        else:\n",
        "                            rhythm_consistency = 0.5\n",
        "                    else:\n",
        "                        rhythm_consistency = 0.5\n",
        "                except Exception:\n",
        "                    rhythm_consistency = 0.5\n",
        "                metrics['rhythmic_consistency'].append(float(rhythm_consistency))\n",
        "\n",
        "                # Melodic coherence with safe pitch tracking\n",
        "                try:\n",
        "                    pitches, magnitudes = librosa.piptrack(y=y, sr=sr, threshold=0.1)\n",
        "                    pitch_track = []\n",
        "                    for t in range(pitches.shape[1]):\n",
        "                        frame_pitches = pitches[:, t]\n",
        "                        frame_mags = magnitudes[:, t]\n",
        "                        valid_indices = frame_mags > 0\n",
        "                        if np.any(valid_indices):\n",
        "                            strongest_pitch_idx = np.argmax(frame_mags)\n",
        "                            if frame_pitches[strongest_pitch_idx] > 0:\n",
        "                                pitch_track.append(frame_pitches[strongest_pitch_idx])\n",
        "\n",
        "                    if len(pitch_track) > 2:\n",
        "                        pitch_changes = np.abs(np.diff(pitch_track))\n",
        "                        if len(pitch_changes) > 0 and np.mean(pitch_changes) > 0:\n",
        "                            melodic_score = 1.0 / (1.0 + np.mean(pitch_changes) / 100.0)\n",
        "                        else:\n",
        "                            melodic_score = 0.5\n",
        "                    else:\n",
        "                        melodic_score = 0.5\n",
        "                except Exception:\n",
        "                    melodic_score = 0.5\n",
        "                metrics['melodic_coherence'].append(float(melodic_score))\n",
        "\n",
        "                # Harmonic progression quality with safe processing\n",
        "                try:\n",
        "                    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "                    if chroma.shape[1] > 1:\n",
        "                        chord_changes = np.sum(np.abs(np.diff(chroma, axis=1)), axis=0)\n",
        "                        if len(chord_changes) > 0 and np.mean(chord_changes) > 0:\n",
        "                            harmonic_smoothness = 1.0 / (1.0 + np.mean(chord_changes))\n",
        "                        else:\n",
        "                            harmonic_smoothness = 0.5\n",
        "                    else:\n",
        "                        harmonic_smoothness = 0.5\n",
        "                except Exception:\n",
        "                    harmonic_smoothness = 0.5\n",
        "                metrics['harmonic_progression_quality'].append(float(harmonic_smoothness))\n",
        "\n",
        "                # Dynamic expression with safe RMS processing\n",
        "                try:\n",
        "                    rms = librosa.feature.rms(y=y)[0]\n",
        "                    if len(rms) > 0 and np.mean(rms) > 0:\n",
        "                        rms_variation = np.std(rms) / (np.mean(rms) + 1e-10)\n",
        "                        dynamic_expression = min(1.0, rms_variation)\n",
        "                    else:\n",
        "                        dynamic_expression = 0.0\n",
        "                except Exception:\n",
        "                    dynamic_expression = 0.0\n",
        "                metrics['dynamic_expression'].append(float(dynamic_expression))\n",
        "\n",
        "                # Phrase structure (musical phrasing) with NaN handling\n",
        "                # Detect phrase boundaries using onset strength and spectral changes\n",
        "                onset_strength = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "                contrast_changes = np.mean(np.abs(np.diff(spectral_contrast, axis=1)), axis=0)\n",
        "\n",
        "                # Good phrasing has clear structural boundaries\n",
        "                if len(onset_strength) > len(contrast_changes):\n",
        "                    onset_strength = onset_strength[:len(contrast_changes)]\n",
        "                elif len(contrast_changes) > len(onset_strength):\n",
        "                    contrast_changes = contrast_changes[:len(onset_strength)]\n",
        "\n",
        "                if len(onset_strength) > 1 and len(contrast_changes) > 1:\n",
        "                    phrase_clarity = np.corrcoef(onset_strength, contrast_changes)[0, 1]\n",
        "                    phrase_score = abs(phrase_clarity) if not np.isnan(phrase_clarity) else 0.5\n",
        "                else:\n",
        "                    phrase_score = 0.5\n",
        "                metrics['phrase_structure'].append(float(phrase_score))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing musical quality for {audio_file}: {e}\")\n",
        "                for key in metrics.keys():\n",
        "                    metrics[key].append(0.5)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_technical_quality(self, audio_files: List[str]) -> Dict[str, List[float]]:\n",
        "        \"\"\"\n",
        "        Evaluate technical audio quality (optimized metrics)\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            'signal_noise_ratio': [],\n",
        "            'dynamic_range': [],\n",
        "            'frequency_response': [],\n",
        "            'artifacts_score': []\n",
        "        }\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            try:\n",
        "                y, sr = librosa.load(audio_file, sr=22050, mono=False)\n",
        "\n",
        "                # Handle mono/stereo\n",
        "                if len(y.shape) == 1:\n",
        "                    y_mono = y\n",
        "                else:\n",
        "                    y_mono = np.mean(y, axis=0)\n",
        "\n",
        "                # Signal-to-noise ratio with safe processing\n",
        "                try:\n",
        "                    signal_power = np.mean(y_mono**2)\n",
        "                    rms = librosa.feature.rms(y=y_mono)[0]\n",
        "\n",
        "                    if len(rms) > 0:\n",
        "                        noise_threshold = np.percentile(rms, 10)\n",
        "                        quiet_sections = rms[rms < noise_threshold]\n",
        "                        if len(quiet_sections) > 0:\n",
        "                            noise_power = np.mean(quiet_sections)**2 + 1e-10\n",
        "                        else:\n",
        "                            noise_power = 1e-10\n",
        "\n",
        "                        if signal_power > 0:\n",
        "                            snr = 10 * np.log10(signal_power / noise_power)\n",
        "                            snr_normalized = min(1.0, max(0.0, snr / 60.0))\n",
        "                        else:\n",
        "                            snr_normalized = 0.0\n",
        "                    else:\n",
        "                        snr_normalized = 0.0\n",
        "                except Exception:\n",
        "                    snr_normalized = 0.0\n",
        "                metrics['signal_noise_ratio'].append(float(snr_normalized))\n",
        "\n",
        "                # Dynamic range with safe processing\n",
        "                try:\n",
        "                    rms = librosa.feature.rms(y=y_mono)[0]\n",
        "                    if len(rms) > 0:\n",
        "                        # Filter out very quiet sections for more meaningful dynamic range\n",
        "                        audible_rms = rms[rms > 1e-6]\n",
        "                        if len(audible_rms) > 0:\n",
        "                            rms_db = 20 * np.log10(audible_rms + 1e-10)\n",
        "                            dynamic_range = np.max(rms_db) - np.min(rms_db)\n",
        "                            dr_normalized = min(1.0, max(0.0, dynamic_range / 60.0))\n",
        "                        else:\n",
        "                            dr_normalized = 0.0\n",
        "                    else:\n",
        "                        dr_normalized = 0.0\n",
        "                except Exception:\n",
        "                    dr_normalized = 0.0\n",
        "                metrics['dynamic_range'].append(float(dr_normalized))\n",
        "\n",
        "                # Frequency response with safe processing\n",
        "                try:\n",
        "                    stft = librosa.stft(y_mono)\n",
        "                    magnitude = np.abs(stft)\n",
        "                    if magnitude.size > 0:\n",
        "                        freq_response = np.mean(magnitude, axis=1)\n",
        "                        # Good piano should have energy across frequency spectrum\n",
        "                        if len(freq_response) > 0 and np.max(freq_response) > 0:\n",
        "                            freq_coverage = np.sum(freq_response > np.max(freq_response) * 0.1) / len(freq_response)\n",
        "                        else:\n",
        "                            freq_coverage = 0.0\n",
        "                    else:\n",
        "                        freq_coverage = 0.0\n",
        "                except Exception:\n",
        "                    freq_coverage = 0.0\n",
        "                metrics['frequency_response'].append(float(freq_coverage))\n",
        "\n",
        "                # Artifacts score with safe processing\n",
        "                try:\n",
        "                    # Check for clipping\n",
        "                    clipping_ratio = np.sum(np.abs(y_mono) > 0.95) / max(len(y_mono), 1)\n",
        "\n",
        "                    # Check for digital artifacts using high-frequency analysis\n",
        "                    stft = librosa.stft(y_mono)\n",
        "                    magnitude = np.abs(stft)\n",
        "                    if magnitude.size > 0:\n",
        "                        mid_point = len(magnitude) // 2\n",
        "                        high_freq_energy = np.mean(magnitude[mid_point:, :]) if mid_point < len(magnitude) else 0\n",
        "                        total_energy = np.mean(magnitude)\n",
        "                        hf_ratio = high_freq_energy / (total_energy + 1e-10) if total_energy > 0 else 0\n",
        "                    else:\n",
        "                        hf_ratio = 0\n",
        "\n",
        "                    # Lower artifacts score is better\n",
        "                    artifacts_penalty = clipping_ratio + min(0.5, hf_ratio)\n",
        "                    artifacts_score = max(0.0, 1.0 - artifacts_penalty)\n",
        "                except Exception:\n",
        "                    artifacts_score = 0.5\n",
        "                metrics['artifacts_score'].append(float(artifacts_score))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing technical quality for {audio_file}: {e}\")\n",
        "                for key in metrics.keys():\n",
        "                    metrics[key].append(0.5)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_semantic_alignment(self, audio_files: List[str]) -> List[float]:\n",
        "        \"\"\"\n",
        "        Evaluate semantic alignment with prompts using multiple approaches\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "\n",
        "        for i, audio_file in enumerate(audio_files):\n",
        "            try:\n",
        "                prompt = self.test_prompts[i] if i < len(self.test_prompts) else self.test_prompts[0]\n",
        "                y, sr = librosa.load(audio_file, sr=22050)\n",
        "\n",
        "                score = 0.5  # Base score\n",
        "                prompt_lower = prompt.lower()\n",
        "\n",
        "                # Tempo analysis with safe beat tracking\n",
        "                try:\n",
        "                    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "                    if tempo is None or tempo <= 0:\n",
        "                        tempo = 100  # Default tempo if detection fails\n",
        "                except Exception:\n",
        "                    tempo = 100\n",
        "\n",
        "                if 'fast' in prompt_lower or 'virtuosic' in prompt_lower:\n",
        "                    if tempo > 120:\n",
        "                        score += 0.15\n",
        "                elif 'slow' in prompt_lower or 'gentle' in prompt_lower or 'peaceful' in prompt_lower:\n",
        "                    if tempo < 100:\n",
        "                        score += 0.15\n",
        "                elif '88 bpm' in prompt_lower:\n",
        "                    if 80 <= tempo <= 96:\n",
        "                        score += 0.2\n",
        "                elif '72 bpm' in prompt_lower:\n",
        "                    if 65 <= tempo <= 80:\n",
        "                        score += 0.2\n",
        "\n",
        "                # Dynamic analysis\n",
        "                rms = librosa.feature.rms(y=y)[0]\n",
        "                avg_energy = np.mean(rms)\n",
        "\n",
        "                if 'forte' in prompt_lower or 'powerful' in prompt_lower or 'dramatic' in prompt_lower:\n",
        "                    if avg_energy > 0.1:\n",
        "                        score += 0.1\n",
        "                elif 'soft' in prompt_lower or 'gentle' in prompt_lower:\n",
        "                    if avg_energy < 0.05:\n",
        "                        score += 0.1\n",
        "\n",
        "                # Style analysis\n",
        "                if 'classical' in prompt_lower or 'baroque' in prompt_lower:\n",
        "                    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "                    harmonic_complexity = np.std(chroma)\n",
        "                    if harmonic_complexity > 0.15:\n",
        "                        score += 0.1\n",
        "\n",
        "                if 'romantic' in prompt_lower or 'emotional' in prompt_lower:\n",
        "                    # Check for expressive dynamics\n",
        "                    rms_variation = np.std(rms) / (np.mean(rms) + 1e-10)\n",
        "                    if rms_variation > 0.3:\n",
        "                        score += 0.1\n",
        "\n",
        "                # Piano timbre check\n",
        "                if 'piano' in prompt_lower:\n",
        "                    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "                    if 800 < spectral_centroid < 3000:  # Typical piano range\n",
        "                        score += 0.1\n",
        "\n",
        "                scores.append(min(1.0, max(0.0, score)))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in semantic alignment for {audio_file}: {e}\")\n",
        "                scores.append(0.5)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def calculate_improvement_focused_scores(self, baseline_metrics: Dict, finetuned_metrics: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate scores that highlight improvements in fine-tuned model\n",
        "        \"\"\"\n",
        "        improvements = {}\n",
        "\n",
        "        # Calculate improvements for each metric category\n",
        "        for category in baseline_metrics.keys():\n",
        "            if category in finetuned_metrics:\n",
        "                improvements[category] = {}\n",
        "\n",
        "                if isinstance(baseline_metrics[category], dict):\n",
        "                    # Nested metrics\n",
        "                    for metric_name in baseline_metrics[category].keys():\n",
        "                        baseline_vals = baseline_metrics[category][metric_name]\n",
        "                        finetuned_vals = finetuned_metrics[category][metric_name]\n",
        "\n",
        "                        baseline_mean = np.mean(baseline_vals)\n",
        "                        finetuned_mean = np.mean(finetuned_vals)\n",
        "\n",
        "                        if baseline_mean != 0:\n",
        "                            improvement_pct = ((finetuned_mean - baseline_mean) / abs(baseline_mean)) * 100\n",
        "                        else:\n",
        "                            improvement_pct = 0.0\n",
        "\n",
        "                        improvements[category][metric_name] = {\n",
        "                            'baseline_mean': float(baseline_mean),\n",
        "                            'finetuned_mean': float(finetuned_mean),\n",
        "                            'improvement_percent': float(improvement_pct),\n",
        "                            'is_improvement': bool(improvement_pct > 0)\n",
        "                        }\n",
        "                elif isinstance(baseline_metrics[category], list):\n",
        "                    # Direct list metrics\n",
        "                    baseline_mean = np.mean(baseline_metrics[category])\n",
        "                    finetuned_mean = np.mean(finetuned_metrics[category])\n",
        "\n",
        "                    if baseline_mean != 0:\n",
        "                        improvement_pct = ((finetuned_mean - baseline_mean) / abs(baseline_mean)) * 100\n",
        "                    else:\n",
        "                        improvement_pct = 0.0\n",
        "\n",
        "                    improvements[category] = {\n",
        "                        'baseline_mean': float(baseline_mean),\n",
        "                        'finetuned_mean': float(finetuned_mean),\n",
        "                        'improvement_percent': float(improvement_pct),\n",
        "                        'is_improvement': bool(improvement_pct > 0)\n",
        "                    }\n",
        "\n",
        "        return improvements\n",
        "\n",
        "    def generate_comparison_table(self, results: Dict) -> str:\n",
        "        \"\"\"\n",
        "        Generate a formatted table comparing baseline and fine-tuned model scores\n",
        "        \"\"\"\n",
        "        table_lines = []\n",
        "        table_lines.append(\"=\" * 80)\n",
        "        table_lines.append(\"DETAILED METRIC COMPARISON TABLE\")\n",
        "        table_lines.append(\"=\" * 80)\n",
        "        table_lines.append(f\"{'Metric Category':<25} {'Metric Name':<25} {'Baseline':<12} {'Fine-tuned':<12} {'Change':<8}\")\n",
        "        table_lines.append(\"-\" * 80)\n",
        "\n",
        "        # Process each category\n",
        "        baseline_data = results['baseline']\n",
        "        finetuned_data = results['finetuned']\n",
        "        improvements = results['improvements']\n",
        "\n",
        "        # Piano Authenticity\n",
        "        if 'piano_authenticity' in baseline_data:\n",
        "            for metric_name in baseline_data['piano_authenticity'].keys():\n",
        "                baseline_mean = np.mean(baseline_data['piano_authenticity'][metric_name])\n",
        "                finetuned_mean = np.mean(finetuned_data['piano_authenticity'][metric_name])\n",
        "                change_pct = improvements['piano_authenticity'][metric_name]['improvement_percent']\n",
        "\n",
        "                table_lines.append(\n",
        "                    f\"{'Piano Authenticity':<25} {metric_name.replace('_', ' ').title():<25} \"\n",
        "                    f\"{baseline_mean:<12.4f} {finetuned_mean:<12.4f} {change_pct:+7.1f}%\"\n",
        "                )\n",
        "\n",
        "        # Musical Quality\n",
        "        if 'musical_quality' in baseline_data:\n",
        "            for metric_name in baseline_data['musical_quality'].keys():\n",
        "                baseline_mean = np.mean(baseline_data['musical_quality'][metric_name])\n",
        "                finetuned_mean = np.mean(finetuned_data['musical_quality'][metric_name])\n",
        "                change_pct = improvements['musical_quality'][metric_name]['improvement_percent']\n",
        "\n",
        "                table_lines.append(\n",
        "                    f\"{'Musical Quality':<25} {metric_name.replace('_', ' ').title():<25} \"\n",
        "                    f\"{baseline_mean:<12.4f} {finetuned_mean:<12.4f} {change_pct:+7.1f}%\"\n",
        "                )\n",
        "\n",
        "        # Technical Quality\n",
        "        if 'technical_quality' in baseline_data:\n",
        "            for metric_name in baseline_data['technical_quality'].keys():\n",
        "                baseline_mean = np.mean(baseline_data['technical_quality'][metric_name])\n",
        "                finetuned_mean = np.mean(finetuned_data['technical_quality'][metric_name])\n",
        "                change_pct = improvements['technical_quality'][metric_name]['improvement_percent']\n",
        "\n",
        "                table_lines.append(\n",
        "                    f\"{'Technical Quality':<25} {metric_name.replace('_', ' ').title():<25} \"\n",
        "                    f\"{baseline_mean:<12.4f} {finetuned_mean:<12.4f} {change_pct:+7.1f}%\"\n",
        "                )\n",
        "\n",
        "        # Semantic Alignment\n",
        "        if 'semantic_alignment' in baseline_data:\n",
        "            baseline_mean = np.mean(baseline_data['semantic_alignment'])\n",
        "            finetuned_mean = np.mean(finetuned_data['semantic_alignment'])\n",
        "            change_pct = improvements['semantic_alignment']['improvement_percent']\n",
        "\n",
        "            table_lines.append(\n",
        "                f\"{'Semantic Alignment':<25} {'Prompt Adherence':<25} \"\n",
        "                f\"{baseline_mean:<12.4f} {finetuned_mean:<12.4f} {change_pct:+7.1f}%\"\n",
        "            )\n",
        "\n",
        "        table_lines.append(\"=\" * 80)\n",
        "\n",
        "        # Save table to file\n",
        "        table_content = \"\\n\".join(table_lines)\n",
        "        table_file = self.output_dir / \"comparison_table.txt\"\n",
        "        with open(table_file, 'w') as f:\n",
        "            f.write(table_content)\n",
        "\n",
        "        return table_content\n",
        "\n",
        "    def run_comprehensive_evaluation(self):\n",
        "        \"\"\"\n",
        "        Run comprehensive evaluation on existing samples\n",
        "        \"\"\"\n",
        "        print(\"Starting Comprehensive Piano Music Evaluation\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Get audio files\n",
        "        baseline_files = self.get_audio_files(self.baseline_dir)\n",
        "        finetuned_files = self.get_audio_files(self.finetuned_dir)\n",
        "\n",
        "        print(f\"Found {len(baseline_files)} baseline samples\")\n",
        "        print(f\"Found {len(finetuned_files)} fine-tuned samples\")\n",
        "\n",
        "        if not baseline_files or not finetuned_files:\n",
        "            print(\"Error: No audio files found in specified directories\")\n",
        "            return None\n",
        "\n",
        "        results = {\n",
        "            'baseline': {},\n",
        "            'finetuned': {},\n",
        "            'improvements': {},\n",
        "            'summary': {},\n",
        "            'metadata': {\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'baseline_samples': len(baseline_files),\n",
        "                'finetuned_samples': len(finetuned_files),\n",
        "                'evaluation_type': 'comprehensive_piano_analysis'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 1. Piano authenticity\n",
        "        print(\"1. Evaluating piano authenticity...\")\n",
        "        baseline_piano = self.evaluate_piano_authenticity(baseline_files)\n",
        "        finetuned_piano = self.evaluate_piano_authenticity(finetuned_files)\n",
        "        results['baseline']['piano_authenticity'] = baseline_piano\n",
        "        results['finetuned']['piano_authenticity'] = finetuned_piano\n",
        "\n",
        "        # 2. Musical quality\n",
        "        print(\"2. Analyzing musical quality...\")\n",
        "        baseline_musical = self.evaluate_musical_quality(baseline_files)\n",
        "        finetuned_musical = self.evaluate_musical_quality(finetuned_files)\n",
        "        results['baseline']['musical_quality'] = baseline_musical\n",
        "        results['finetuned']['musical_quality'] = finetuned_musical\n",
        "\n",
        "        # 3. Technical quality\n",
        "        print(\"3. Assessing technical quality...\")\n",
        "        baseline_technical = self.evaluate_technical_quality(baseline_files)\n",
        "        finetuned_technical = self.evaluate_technical_quality(finetuned_files)\n",
        "        results['baseline']['technical_quality'] = baseline_technical\n",
        "        results['finetuned']['technical_quality'] = finetuned_technical\n",
        "\n",
        "        # 4. Semantic alignment\n",
        "        print(\"4. Evaluating semantic alignment...\")\n",
        "        baseline_semantic = self.evaluate_semantic_alignment(baseline_files)\n",
        "        finetuned_semantic = self.evaluate_semantic_alignment(finetuned_files)\n",
        "        results['baseline']['semantic_alignment'] = baseline_semantic\n",
        "        results['finetuned']['semantic_alignment'] = finetuned_semantic\n",
        "\n",
        "        # 5. Calculate improvements\n",
        "        print(\"5. Calculating improvement metrics...\")\n",
        "        improvements = self.calculate_improvement_focused_scores(\n",
        "            results['baseline'], results['finetuned']\n",
        "        )\n",
        "        results['improvements'] = improvements\n",
        "\n",
        "        # 6. Generate summary\n",
        "        summary = self.generate_improvement_summary(results)\n",
        "        results['summary'] = summary\n",
        "\n",
        "        # 7. Generate comparison table\n",
        "        print(\"6. Generating comparison table...\")\n",
        "        comparison_table = self.generate_comparison_table(results)\n",
        "\n",
        "        # Save results with proper JSON serialization\n",
        "        results_file = self.output_dir / \"comprehensive_evaluation_results.json\"\n",
        "\n",
        "        # Convert numpy types to Python types for JSON serialization\n",
        "        def convert_numpy_types(obj):\n",
        "            \"\"\"Recursively convert numpy types to Python types\"\"\"\n",
        "            if isinstance(obj, dict):\n",
        "                return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_numpy_types(item) for item in obj]\n",
        "            elif isinstance(obj, np.integer):\n",
        "                return int(obj)\n",
        "            elif isinstance(obj, np.floating):\n",
        "                return float(obj)\n",
        "            elif isinstance(obj, np.bool_):\n",
        "                return bool(obj)\n",
        "            elif isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            elif hasattr(obj, 'item'):  # Handle numpy scalars\n",
        "                return obj.item()\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        # Convert results to JSON-serializable format\n",
        "        json_results = convert_numpy_types(results)\n",
        "\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(json_results, f, indent=2)\n",
        "\n",
        "        # Print comparison table\n",
        "        print(\"\\n\" + comparison_table)\n",
        "\n",
        "        print(f\"\\n✓ Comprehensive evaluation complete!\")\n",
        "        print(f\"Results saved to {results_file}\")\n",
        "        print(f\"Comparison table saved to comparison_table.txt\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_improvement_summary(self, results: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate a summary highlighting improvements\n",
        "        \"\"\"\n",
        "        improvements = results['improvements']\n",
        "\n",
        "        summary = {\n",
        "            'total_metrics_evaluated': 0,\n",
        "            'metrics_improved': 0,\n",
        "            'significant_improvements': [],\n",
        "            'overall_improvement_score': 0.0,\n",
        "            'key_findings': []\n",
        "        }\n",
        "\n",
        "        # Count improvements\n",
        "        for category, metrics in improvements.items():\n",
        "            if isinstance(metrics, dict) and 'baseline_mean' in metrics:\n",
        "                # Single metric\n",
        "                summary['total_metrics_evaluated'] += 1\n",
        "                if metrics['is_improvement']:\n",
        "                    summary['metrics_improved'] += 1\n",
        "                    if metrics['improvement_percent'] > 5:\n",
        "                        summary['significant_improvements'].append({\n",
        "                            'metric': category,\n",
        "                            'improvement': metrics['improvement_percent']\n",
        "                        })\n",
        "            else:\n",
        "                # Nested metrics\n",
        "                for metric_name, metric_data in metrics.items():\n",
        "                    if isinstance(metric_data, dict) and 'baseline_mean' in metric_data:\n",
        "                        summary['total_metrics_evaluated'] += 1\n",
        "                        if metric_data['is_improvement']:\n",
        "                            summary['metrics_improved'] += 1\n",
        "                            if metric_data['improvement_percent'] > 5:\n",
        "                                summary['significant_improvements'].append({\n",
        "                                    'metric': f\"{category}.{metric_name}\",\n",
        "                                    'improvement': metric_data['improvement_percent']\n",
        "                                })\n",
        "\n",
        "        # Calculate overall improvement score\n",
        "        if summary['total_metrics_evaluated'] > 0:\n",
        "            summary['overall_improvement_score'] = (\n",
        "                summary['metrics_improved'] / summary['total_metrics_evaluated']\n",
        "            ) * 100\n",
        "\n",
        "        # Generate key findings\n",
        "        if summary['metrics_improved'] > summary['total_metrics_evaluated'] * 0.6:\n",
        "            summary['key_findings'].append(\"Fine-tuned model shows improvements across majority of metrics\")\n",
        "\n",
        "        if len(summary['significant_improvements']) > 0:\n",
        "            summary['key_findings'].append(f\"Significant improvements in {len(summary['significant_improvements'])} key areas\")\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def generate_comprehensive_report(self, results: Dict):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive evaluation report\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"COMPREHENSIVE PIANO MUSIC GENERATION EVALUATION REPORT\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Overall summary\n",
        "        summary = results['summary']\n",
        "        print(f\"\\nOVERALL SUMMARY:\")\n",
        "        print(f\"Total Metrics Evaluated: {summary['total_metrics_evaluated']}\")\n",
        "        print(f\"Metrics Showing Improvement: {summary['metrics_improved']}\")\n",
        "        print(f\"Overall Improvement Score: {summary['overall_improvement_score']:.1f}%\")\n",
        "\n",
        "        # Significant improvements\n",
        "        if summary['significant_improvements']:\n",
        "            print(f\"\\nSIGNIFICANT IMPROVEMENTS (>5%):\")\n",
        "            for improvement in sorted(summary['significant_improvements'],\n",
        "                                   key=lambda x: x['improvement'], reverse=True):\n",
        "                print(f\"  ✓ {improvement['metric']}: +{improvement['improvement']:.1f}%\")\n",
        "\n",
        "        # Category-wise analysis\n",
        "        improvements = results['improvements']\n",
        "\n",
        "        print(f\"\\nPIANO AUTHENTICITY ANALYSIS:\")\n",
        "        if 'piano_authenticity' in improvements:\n",
        "            for metric, data in improvements['piano_authenticity'].items():\n",
        "                status = \"↗\" if data['is_improvement'] else \"↘\"\n",
        "                print(f\"  {status} {metric}: {data['baseline_mean']:.3f} → {data['finetuned_mean']:.3f} ({data['improvement_percent']:+.1f}%)\")\n",
        "\n",
        "        print(f\"\\nMUSICAL QUALITY ANALYSIS:\")\n",
        "        if 'musical_quality' in improvements:\n",
        "            for metric, data in improvements['musical_quality'].items():\n",
        "                status = \"↗\" if data['is_improvement'] else \"↘\"\n",
        "                print(f\"  {status} {metric}: {data['baseline_mean']:.3f} → {data['finetuned_mean']:.3f} ({data['improvement_percent']:+.1f}%)\")\n",
        "\n",
        "        print(f\"\\nTECHNICAL QUALITY ANALYSIS:\")\n",
        "        if 'technical_quality' in improvements:\n",
        "            for metric, data in improvements['technical_quality'].items():\n",
        "                status = \"↗\" if data['is_improvement'] else \"↘\"\n",
        "                print(f\"  {status} {metric}: {data['baseline_mean']:.3f} → {data['finetuned_mean']:.3f} ({data['improvement_percent']:+.1f}%)\")\n",
        "\n",
        "        print(f\"\\nSEMANTIC ALIGNMENT:\")\n",
        "        if 'semantic_alignment' in improvements:\n",
        "            data = improvements['semantic_alignment']\n",
        "            status = \"↗\" if data['is_improvement'] else \"↘\"\n",
        "            print(f\"  {status} Prompt Adherence: {data['baseline_mean']:.3f} → {data['finetuned_mean']:.3f} ({data['improvement_percent']:+.1f}%)\")\n",
        "\n",
        "        # Key findings\n",
        "        print(f\"\\nKEY FINDINGS:\")\n",
        "        for finding in summary['key_findings']:\n",
        "            print(f\"  • {finding}\")\n",
        "\n",
        "        # Recommendations\n",
        "        print(f\"\\nRECOMMENDAT_IONS:\")\n",
        "        if summary['overall_improvement_score'] > 60:\n",
        "            print(\"  ✅ Fine-tuning has been successful - model shows clear improvements\")\n",
        "        elif summary['overall_improvement_score'] > 40:\n",
        "            print(\"  🔶 Fine-tuning shows moderate improvements - consider additional training\")\n",
        "        else:\n",
        "            print(\"  ⚠️  Fine-tuning shows limited improvements - review training strategy\")\n",
        "\n",
        "        if len(summary['significant_improvements']) > 3:\n",
        "            print(\"  ✅ Strong improvements across multiple quality dimensions\")\n",
        "\n",
        "        if fad_metrics.get('fad_baseline_vs_finetuned', 100) < 10:\n",
        "            print(\"  ✅ Low FAD indicates successful quality transfer\")\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run comprehensive evaluation\n",
        "    \"\"\"\n",
        "    # Sample directories\n",
        "    baseline_dir = \"evaluation_results/baseline_samples\"\n",
        "    finetuned_dir = \"evaluation_results/finetuned_samples\"\n",
        "\n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(baseline_dir):\n",
        "        print(f\"Error: Baseline samples directory not found: {baseline_dir}\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(finetuned_dir):\n",
        "        print(f\"Error: Fine-tuned samples directory not found: {finetuned_dir}\")\n",
        "        return None\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = EnhancedPianoEvaluator(\n",
        "        baseline_dir=baseline_dir,\n",
        "        finetuned_dir=finetuned_dir\n",
        "    )\n",
        "\n",
        "    # Run comprehensive evaluation\n",
        "    results = evaluator.run_comprehensive_evaluation()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nComprehensive evaluation completed successfully!\")\n",
        "\n",
        "        # Create summary visualization\n",
        "        create_improvement_visualization(results)\n",
        "\n",
        "        return results\n",
        "    else:\n",
        "        print(\"\\nEvaluation failed!\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def create_improvement_visualization(results: Dict):\n",
        "    \"\"\"\n",
        "    Create visualization showing improvements\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Collect improvement percentages\n",
        "        improvements_data = []\n",
        "        labels = []\n",
        "\n",
        "        for category, metrics in results['improvements'].items():\n",
        "            if isinstance(metrics, dict) and 'improvement_percent' in metrics:\n",
        "                improvements_data.append(metrics['improvement_percent'])\n",
        "                labels.append(category.replace('_', ' ').title())\n",
        "            else:\n",
        "                for metric_name, metric_data in metrics.items():\n",
        "                    if isinstance(metric_data, dict) and 'improvement_percent' in metric_data:\n",
        "                        improvements_data.append(metric_data['improvement_percent'])\n",
        "                        labels.append(f\"{category.replace('_', ' ').title()}\\n{metric_name.replace('_', ' ')}\")\n",
        "\n",
        "        if improvements_data:\n",
        "            # Create horizontal bar chart\n",
        "            fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "            colors = ['green' if x > 0 else 'red' for x in improvements_data]\n",
        "            bars = ax.barh(range(len(improvements_data)), improvements_data, color=colors, alpha=0.7)\n",
        "\n",
        "            ax.set_yticks(range(len(labels)))\n",
        "            ax.set_yticklabels(labels, fontsize=10)\n",
        "            ax.set_xlabel('Improvement Percentage (%)', fontsize=12)\n",
        "            ax.set_title('Fine-tuned Model Improvements by Metric', fontsize=14, fontweight='bold')\n",
        "            ax.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
        "            ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for i, (bar, value) in enumerate(zip(bars, improvements_data)):\n",
        "                ax.text(value + (1 if value > 0 else -1), i, f'{value:.1f}%',\n",
        "                       va='center', ha='left' if value > 0 else 'right', fontsize=9)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('evaluation_results/results/improvement_visualization.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            print(\"✓ Improvement visualization saved to evaluation_results/results/improvement_visualization.png\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not available - skipping visualization\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualization: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6heLoxLVFMtV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}